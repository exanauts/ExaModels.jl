var documenterSearchIndex = {"docs":
[{"location":"develop/#Developing-Extensions","page":"Developing Extensions","title":"Developing Extensions","text":"ExaModels.jl's API only uses simple julia functions, and thus, implementing the extensions is straightforward. Below, we suggest a good practice for implementing an extension package.\n\nLet's say that we want to implement an extension package for the example problem in Getting Started. An extension package may look like:\n\nRoot\n├───Project.toml\n├── src\n│   └── LuksanVlcekModels.jl\n└── test\n    └── runtest.jl\n\nEach of the files containing\n\n# Project.toml\n\nname = \"LuksanVlcekModels\"\nuuid = \"0c5951a0-f777-487f-ad29-fac2b9a21bf1\"\nauthors = [\"Sungho Shin <sshin@anl.gov>\"]\nversion = \"0.1.0\"\n\n[deps]\nExaModels = \"1037b233-b668-4ce9-9b63-f9f681f55dd2\"\n\n[extras]\nNLPModelsIpopt = \"f4238b75-b362-5c4c-b852-0801c9a21d71\"\nTest = \"8dfed614-e22c-5e08-85e1-65c5234f0b40\"\n\n[targets]\ntest = [\"Test\", \"NLPModelsIpopt\"]\n\n# src/LuksanVlcekModels.jl\n\nmodule LuksanVlcekModels\n\nimport ExaModels\n\nfunction luksan_vlcek_obj(x,i)\n    return 100*(x[i-1]^2-x[i])^2+(x[i-1]-1)^2\nend\n\nfunction luksan_vlcek_con(x,i)\n    return 3x[i+1]^3+2*x[i+2]-5+sin(x[i+1]-x[i+2])sin(x[i+1]+x[i+2])+4x[i+1]-x[i]exp(x[i]-x[i+1])-3\nend\n\nfunction luksan_vlcek_x0(i)\n    return mod(i,2)==1 ? -1.2 : 1.0\nend\n\nfunction luksan_vlcek_model(N; backend = nothing)\n    \n    c = ExaModels.ExaCore(backend)\n    x = ExaModels.variable(\n        c, N;\n        start = (luksan_vlcek_x0(i) for i=1:N)\n    )\n    ExaModels.constraint(\n        c,\n        luksan_vlcek_con(x,i)\n        for i in 1:N-2)\n    ExaModels.objective(c, luksan_vlcek_obj(x,i) for i in 2:N)\n    \n    return ExaModels.ExaModel(c) # returns the model\nend\n\nexport luksan_vlcek_model\n\nend # module LuksanVlcekModels\n\n# test/runtest.jl\n\nusing Test, LuksanVlcekModels, NLPModelsIpopt\n\n@testset \"LuksanVlcekModelsTest\" begin\n    m = luksan_vlcek_model(10)\n    result = ipopt(m)\n\n    @test result.status == :first_order\n    @test result.solution ≈ [\n        -0.9505563573613093\n        0.9139008176388945\n        0.9890905176644905\n        0.9985592422681151\n        0.9998087408802769\n        0.9999745932450963\n        0.9999966246997642\n        0.9999995512524277\n        0.999999944919307\n        0.999999930070643\n    ]\n    @test result.multipliers ≈ [\n        4.1358568305002255\n        -1.876494903703342\n        -0.06556333356358675\n        -0.021931863018312875\n        -0.0019537261317119302\n        -0.00032910445671233547\n        -3.8788212776372465e-5\n        -7.376592164341867e-6\n    ]\nend","category":"section"},{"location":"distillation/#distillation","page":"Example: Distillation Column","title":"Example: Distillation Column","text":"This example demonstrates the use of subexpr to simplify complex models. We show three versions of a distillation column model comparing lifted vs reduced subexpressions.","category":"section"},{"location":"distillation/#Subexpressions-in-ExaModels","page":"Example: Distillation Column","title":"Subexpressions in ExaModels","text":"ExaModels provides the subexpr function to define reusable expressions in two forms:\n\nLifted subexpressions (default): Creates auxiliary variables with defining equality constraints.\n\ns = subexpr(c, x[i]^2 for i in 1:n)  # adds n variables + n constraints\n\nReduced subexpressions: Inlines the expression directly, no extra variables or constraints.\n\ns = subexpr(c, x[i]^2 for i in 1:n; reduced=true)  # no extra vars/cons","category":"section"},{"location":"distillation/#Trade-offs:-Lifted-vs-Reduced","page":"Example: Distillation Column","title":"Trade-offs: Lifted vs Reduced","text":"Aspect Lifted Reduced\nProblem size Adds auxiliary vars/cons No extra vars/cons\nExpression complexity Simple variable references Inlined expressions\nDerivative code Generated once per pattern Regenerated at each use","category":"section"},{"location":"distillation/#Performance-Comparison-(T10)","page":"Example: Distillation Column","title":"Performance Comparison (T=10)","text":"Model Variables Constraints Iterations\nOriginal (no subexpr) 737 726 7\nLifted 1398 1387 7\nReduced 737 726 7\n\nAll three models converge to the same optimal solution. Reduced subexpressions maintain the original problem size while providing the code clarity benefits of subexpressions.","category":"section"},{"location":"distillation/#Original-Model-(without-subexpressions)","page":"Example: Distillation Column","title":"Original Model (without subexpressions)","text":"This is the original formulation where expressions like (xA[t, i] - xA[t-1, i]) / dt are repeated in multiple constraints.\n\nfunction distillation_column_model(T = 3; backend = nothing)\n\n    NT = 30\n    FT = 17\n    Ac = 0.5\n    At = 0.25\n    Ar = 1.0\n    D = 0.2\n    F = 0.4\n    ybar = 0.8958\n    ubar = 2.0\n    alpha = 1.6\n    dt = 10 / T\n    xAf = 0.5\n    xA0s = ExaModels.convert_array([(i, 0.5) for i = 0:(NT+1)], backend)\n\n    itr0 = ExaModels.convert_array(collect(Iterators.product(1:T, 1:(FT-1))), backend)\n    itr1 = ExaModels.convert_array(collect(Iterators.product(1:T, (FT+1):NT)), backend)\n    itr2 = ExaModels.convert_array(collect(Iterators.product(0:T, 0:(NT+1))), backend)\n\n    c = ExaCore(backend)\n\n    xA = variable(c, 0:T, 0:(NT+1); start = 0.5)\n    yA = variable(c, 0:T, 0:(NT+1); start = 0.5)\n    u = variable(c, 0:T; start = 1.0)\n    V = variable(c, 0:T; start = 1.0)\n    L2 = variable(c, 0:T; start = 1.0)\n\n    objective(c, (yA[t, 1] - ybar)^2 for t = 0:T)\n    objective(c, (u[t] - ubar)^2 for t = 0:T)\n\n    constraint(c, xA[0, i] - xA0 for (i, xA0) in xA0s)\n    constraint(\n        c,\n        (xA[t, 0] - xA[t-1, 0]) / dt - (1 / Ac) * (yA[t, 1] - xA[t, 0]) for t = 1:T\n    )\n    constraint(\n        c,\n        (xA[t, i] - xA[t-1, i]) / dt -\n        (1 / At) * (u[t] * D * (yA[t, i-1] - xA[t, i]) - V[t] * (yA[t, i] - yA[t, i+1])) for\n        (t, i) in itr0\n    )\n    constraint(\n        c,\n        (xA[t, FT] - xA[t-1, FT]) / dt -\n        (1 / At) * (\n            F * xAf + u[t] * D * xA[t, FT-1] - L2[t] * xA[t, FT] -\n            V[t] * (yA[t, FT] - yA[t, FT+1])\n        ) for t = 1:T\n    )\n    constraint(\n        c,\n        (xA[t, i] - xA[t-1, i]) / dt -\n        (1 / At) * (L2[t] * (yA[t, i-1] - xA[t, i]) - V[t] * (yA[t, i] - yA[t, i+1])) for\n        (t, i) in itr1\n    )\n    constraint(\n        c,\n        (xA[t, NT+1] - xA[t-1, NT+1]) / dt -\n        (1 / Ar) * (L2[t] * xA[t, NT] - (F - D) * xA[t, NT+1] - V[t] * yA[t, NT+1]) for\n        t = 1:T\n    )\n    constraint(c, V[t] - u[t] * D - D for t = 0:T)\n    constraint(c, L2[t] - u[t] * D - F for t = 0:T)\n    constraint(\n        c,\n        yA[t, i] * (1 - xA[t, i]) - alpha * xA[t, i] * (1 - yA[t, i]) for (t, i) in itr2\n    )\n\n    return ExaModel(c)\nend\n\ndistillation_column_model (generic function with 2 methods)","category":"section"},{"location":"distillation/#Model-with-Lifted-Subexpressions","page":"Example: Distillation Column","title":"Model with Lifted Subexpressions","text":"Uses subexpressions for time derivatives and vapor differences. This adds auxiliary variables and constraints but makes the model more readable.\n\nfunction distillation_column_model_with_subexpr(T = 3; backend = nothing)\n\n    NT = 30\n    FT = 17\n    Ac = 0.5\n    At = 0.25\n    Ar = 1.0\n    D = 0.2\n    F = 0.4\n    ybar = 0.8958\n    ubar = 2.0\n    alpha = 1.6\n    dt = 10 / T\n    xAf = 0.5\n    xA0s = ExaModels.convert_array([(i, 0.5) for i in 0:(NT + 1)], backend)\n\n    c = ExaCore(backend)\n\n    # Decision variables\n    xA = variable(c, 0:T, 0:(NT + 1); start = 0.5)\n    yA = variable(c, 0:T, 0:(NT + 1); start = 0.5)\n    u = variable(c, 0:T; start = 1.0)\n    V = variable(c, 0:T; start = 1.0)\n    L2 = variable(c, 0:T; start = 1.0)\n\n    # Subexpressions - define common terms once\n    dxA = subexpr(c, (xA[t, i] - xA[t - 1, i]) / dt for t in 1:T, i in 0:(NT + 1))\n    dyA = subexpr(c, yA[t, i] - yA[t, i + 1] for t in 0:T, i in 0:NT)\n\n    # Objectives\n    objective(c, (yA[t, 1] - ybar)^2 for t in 0:T)\n    objective(c, (u[t] - ubar)^2 for t in 0:T)\n\n    # Initial conditions\n    constraint(c, xA[0, i] - xA0 for (i, xA0) in xA0s)\n\n    # Condenser - now using dxA subexpression\n    constraint(c, dxA[t, 0] - (1 / Ac) * (yA[t, 1] - xA[t, 0]) for t in 1:T)\n\n    # Rectifying section - cleaner with dxA and dyA\n    itr_rect = ExaModels.convert_array(collect(Iterators.product(1:T, 1:(FT - 1))), backend)\n    constraint(c, dxA[t, i] - (1 / At) * (u[t] * D * (yA[t, i - 1] - xA[t, i]) - V[t] * dyA[t, i]) for (t, i) in itr_rect)\n\n    # Feed tray\n    constraint(c, dxA[t, FT] - (1 / At) * (F * xAf + u[t] * D * xA[t, FT - 1] - L2[t] * xA[t, FT] - V[t] * dyA[t, FT]) for t in 1:T)\n\n    # Stripping section\n    itr_strip = ExaModels.convert_array(collect(Iterators.product(1:T, (FT + 1):NT)), backend)\n    constraint(c, dxA[t, i] - (1 / At) * (L2[t] * (yA[t, i - 1] - xA[t, i]) - V[t] * dyA[t, i]) for (t, i) in itr_strip)\n\n    # Reboiler\n    constraint(c, dxA[t, NT + 1] - (1 / Ar) * (L2[t] * xA[t, NT] - (F - D) * xA[t, NT + 1] - V[t] * yA[t, NT + 1]) for t in 1:T)\n\n    # Flow relationships\n    constraint(c, V[t] - u[t] * D - D for t in 0:T)\n    constraint(c, L2[t] - u[t] * D - F for t in 0:T)\n\n    # VLE\n    itr_vle = ExaModels.convert_array(collect(Iterators.product(0:T, 0:(NT + 1))), backend)\n    constraint(c, yA[t, i] * (1 - xA[t, i]) - alpha * xA[t, i] * (1 - yA[t, i]) for (t, i) in itr_vle)\n\n    return ExaModel(c)\nend\n\ndistillation_column_model_with_subexpr (generic function with 2 methods)","category":"section"},{"location":"distillation/#Model-with-Reduced-Subexpressions","page":"Example: Distillation Column","title":"Model with Reduced Subexpressions","text":"Uses reduced subexpressions for time derivatives and vapor differences. Unlike lifted subexpressions, reduced subexpressions do NOT add auxiliary variables or constraints - they inline the expression directly where used.\n\nThis gives the same problem size as the original model but with cleaner code.\n\nfunction distillation_column_model_reduced(T = 3; backend = nothing)\n\n    NT = 30\n    FT = 17\n    Ac = 0.5\n    At = 0.25\n    Ar = 1.0\n    D = 0.2\n    F = 0.4\n    ybar = 0.8958\n    ubar = 2.0\n    alpha = 1.6\n    dt = 10 / T\n    xAf = 0.5\n    xA0s = ExaModels.convert_array([(i, 0.5) for i in 0:(NT + 1)], backend)\n\n    c = ExaCore(backend)\n\n    # Decision variables (same as original)\n    xA = variable(c, 0:T, 0:(NT + 1); start = 0.5)\n    yA = variable(c, 0:T, 0:(NT + 1); start = 0.5)\n    u = variable(c, 0:T; start = 1.0)\n    V = variable(c, 0:T; start = 1.0)\n    L2 = variable(c, 0:T; start = 1.0)\n\n    # Reduced subexpressions - NO extra variables or constraints!\n    # These inline the expression wherever they're used\n    dxA = subexpr(c, (xA[t, i] - xA[t - 1, i]) / dt for t in 1:T, i in 0:(NT + 1); reduced = true)\n    dyA = subexpr(c, yA[t, i] - yA[t, i + 1] for t in 0:T, i in 0:NT; reduced = true)\n\n    # Objectives\n    objective(c, (yA[t, 1] - ybar)^2 for t in 0:T)\n    objective(c, (u[t] - ubar)^2 for t in 0:T)\n\n    # Initial conditions\n    constraint(c, xA[0, i] - xA0 for (i, xA0) in xA0s)\n\n    # Condenser - using dxA reduced subexpression (inlines the derivative)\n    constraint(c, dxA[t, 0] - (1 / Ac) * (yA[t, 1] - xA[t, 0]) for t in 1:T)\n\n    # Rectifying section\n    itr_rect = ExaModels.convert_array(collect(Iterators.product(1:T, 1:(FT - 1))), backend)\n    constraint(c, dxA[t, i] - (1 / At) * (u[t] * D * (yA[t, i - 1] - xA[t, i]) - V[t] * dyA[t, i]) for (t, i) in itr_rect)\n\n    # Feed tray\n    constraint(c, dxA[t, FT] - (1 / At) * (F * xAf + u[t] * D * xA[t, FT - 1] - L2[t] * xA[t, FT] - V[t] * dyA[t, FT]) for t in 1:T)\n\n    # Stripping section\n    itr_strip = ExaModels.convert_array(collect(Iterators.product(1:T, (FT + 1):NT)), backend)\n    constraint(c, dxA[t, i] - (1 / At) * (L2[t] * (yA[t, i - 1] - xA[t, i]) - V[t] * dyA[t, i]) for (t, i) in itr_strip)\n\n    # Reboiler\n    constraint(c, dxA[t, NT + 1] - (1 / Ar) * (L2[t] * xA[t, NT] - (F - D) * xA[t, NT + 1] - V[t] * yA[t, NT + 1]) for t in 1:T)\n\n    # Flow relationships (still needed as these are decision variables)\n    constraint(c, V[t] - u[t] * D - D for t in 0:T)\n    constraint(c, L2[t] - u[t] * D - F for t in 0:T)\n\n    # VLE\n    itr_vle = ExaModels.convert_array(collect(Iterators.product(0:T, 0:(NT + 1))), backend)\n    constraint(c, yA[t, i] * (1 - xA[t, i]) - alpha * xA[t, i] * (1 - yA[t, i]) for (t, i) in itr_vle)\n\n    return ExaModel(c)\nend\n\ndistillation_column_model_reduced (generic function with 2 methods)","category":"section"},{"location":"distillation/#Running-the-Models","page":"Example: Distillation Column","title":"Running the Models","text":"Let's compare all three model variants and verify they converge to the same solution.\n\nusing ExaModels, NLPModelsIpopt\n\nT_val = 10\n\n10\n\nOriginal model (no subexpressions)\n\nm_orig = distillation_column_model(T_val)\nresult_orig = ipopt(m_orig; print_level = 0)\n\n\"Execution stats: first-order stationary\"\n\nLifted subexpressions\n\nm_lifted = distillation_column_model_with_subexpr(T_val)\nresult_lifted = ipopt(m_lifted; print_level = 0)\n\n\"Execution stats: first-order stationary\"\n\nReduced subexpressions\n\nm_reduced = distillation_column_model_reduced(T_val)\nresult_reduced = ipopt(m_reduced; print_level = 0)\n\n\"Execution stats: first-order stationary\"","category":"section"},{"location":"distillation/#Comparison-Results","page":"Example: Distillation Column","title":"Comparison Results","text":"Print comparison table:\n\nprintln(\"=\"^70)\nprintln(\"Distillation Column Model Comparison (T=$T_val)\")\nprintln(\"=\"^70)\nprintln()\nprintln(\"| Model                  | Variables | Constraints | Iterations | Objective |\")\nprintln(\"|------------------------|-----------|-------------|------------|-----------|\")\nprintln(\"| Original (no subexpr)  | $(lpad(m_orig.meta.nvar, 9)) | $(lpad(m_orig.meta.ncon, 11)) | $(lpad(result_orig.iter, 10)) | $(round(result_orig.objective, digits = 6)) |\")\nprintln(\"| Lifted                 | $(lpad(m_lifted.meta.nvar, 9)) | $(lpad(m_lifted.meta.ncon, 11)) | $(lpad(result_lifted.iter, 10)) | $(round(result_lifted.objective, digits = 6)) |\")\nprintln(\"| Reduced                | $(lpad(m_reduced.meta.nvar, 9)) | $(lpad(m_reduced.meta.ncon, 11)) | $(lpad(result_reduced.iter, 10)) | $(round(result_reduced.objective, digits = 6)) |\")\nprintln()\n\n======================================================================\nDistillation Column Model Comparison (T=10)\n======================================================================\n\n| Model                  | Variables | Constraints | Iterations | Objective |\n|------------------------|-----------|-------------|------------|-----------|\n| Original (no subexpr)  |       737 |         726 |          7 | 0.15107 |\n| Lifted                 |      1398 |        1387 |          7 | 0.15107 |\n| Reduced                |       737 |         726 |          7 | 0.15107 |\n\n\n\nVerify all models converge to the same solution:\n\nall_same = all(\n    [\n        isapprox(result_orig.objective, result_lifted.objective, rtol = 1.0e-4),\n        isapprox(result_orig.objective, result_reduced.objective, rtol = 1.0e-4),\n    ]\n)\n\nprintln(\"All models converge to same objective: $all_same\")\n\nAll models converge to same objective: true\n\n\nKey observations:\n\nLifted subexpressions ADD variables/constraints (auxiliary vars + defining constraints)\nReduced subexpressions maintain the original problem size (no aux vars)\nAll models reach the same optimal solution\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"parameters/#parameters","page":"Parameters","title":"Parameters","text":"Parameters act like fixed variables. Internally, ExaModels keeps track of where parameters appear in the model, making it possible to efficiently modify their value without rebuilding the entire model.","category":"section"},{"location":"parameters/#Creating-Parametric-Models","page":"Parameters","title":"Creating Parametric Models","text":"Let's modify the example in Getting Started to use parameters. Suppose we want to make the penalty coefficient in the objective function adjustable:\n\nFirst, let's create a core:\n\nusing ExaModels, NLPModelsIpopt\nc_param = ExaCore()\n\nAn ExaCore\n\n  Float type: ...................... Float64\n  Array type: ...................... Vector{Float64}\n  Backend: ......................... Nothing\n\n  number of objective patterns: .... 0\n  number of constraint patterns: ... 0\n\n\nAdding parameters is very similar to adding variables – just pass a vector of values denoting the initial values.\n\nθ = parameter(c_param, [100.0, 1.0])  # [penalty_coeff, offset]\n\nParameter\n\n  θ ∈ R^{2}\n\n\nDefine the variables as before:\n\nN = 10\nx_p = variable(c_param, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N))\n\nVariable\n\n  x ∈ R^{10}\n\n\nNow we can use the parameters in our objective function just like variables:\n\nobjective(c_param, θ[1] * (x_p[i-1]^2 - x_p[i])^2 + (x_p[i-1] - θ[2])^2 for i = 2:N)\n\nObjective\n\n  min (...) + ∑_{p ∈ P} f(x,θ,p)\n\n  where |P| = 9\n\n\nAdd the same constraints as before:\n\nconstraint(\n    c_param,\n    3x_p[i+1]^3 + 2 * x_p[i+2] - 5 +\n    sin(x_p[i+1] - x_p[i+2])sin(x_p[i+1] + x_p[i+2]) +\n    4x_p[i+1] - x_p[i]exp(x_p[i] - x_p[i+1]) - 3 for i = 1:(N-2)\n)\n\nConstraint\n\n  s.t. (...)\n       g♭ ≤ [g(x,θ,p)]_{p ∈ P} ≤ g♯\n\n  where |P| = 8\n\n\nCreate the model as before:\n\nm_param = ExaModel(c_param)\n\nAn ExaModel{Float64, Vector{Float64}, ...}\n\n  Problem name: Generic\n   All variables: ████████████████████ 10     All constraints: ████████████████████ 8     \n            free: ████████████████████ 10                free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ████████████████████ 8     \n          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n            nnzh: (-36.36% sparsity)   75              linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n                                                    nonlinear: ████████████████████ 8     \n                                                         nnzj: ( 70.00% sparsity)   24    \n                                                     lin_nnzj: (------% sparsity)         \n                                                     nln_nnzj: ( 70.00% sparsity)   24    \n\n\n\nSolve with original parameters:\n\nresult1 = ipopt(m_param)\nprintln(\"Original objective: $(result1.objective)\")\n\nThis is Ipopt version 3.14.19, running with linear solver MUMPS 5.8.2.\n\nNumber of nonzeros in equality constraint Jacobian...:       24\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       75\n\nTotal number of variables............................:       10\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        8\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  2.0570000e+03 2.48e+01 2.73e+01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n   1  1.0953147e+03 1.49e+01 8.27e+01  -1.0 2.20e+00    -  1.00e+00 1.00e+00f  1\n   2  3.2865521e+02 4.28e+00 1.36e+02  -1.0 1.43e+00    -  1.00e+00 1.00e+00f  1\n   3  1.3995370e+01 3.09e-01 2.18e+01  -1.0 5.63e-01    -  1.00e+00 1.00e+00f  1\n   4  6.2325715e+00 1.73e-02 8.47e-01  -1.0 2.10e-01    -  1.00e+00 1.00e+00f  1\n   5  6.2324586e+00 1.15e-05 8.16e-04  -1.7 3.35e-03    -  1.00e+00 1.00e+00h  1\n   6  6.2324586e+00 8.35e-12 7.97e-10  -5.7 2.00e-06    -  1.00e+00 1.00e+00h  1\n\nNumber of Iterations....: 6\n\n                                   (scaled)                 (unscaled)\nObjective...............:   7.8692659500473017e-01    6.2324586324374636e+00\nDual infeasibility......:   7.9746955363607132e-10    6.3159588647976857e-09\nConstraint violation....:   8.3546503049092280e-12    8.3546503049092280e-12\nVariable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   7.9746955363607132e-10    6.3159588647976857e-09\n\n\nNumber of objective function evaluations             = 7\nNumber of objective gradient evaluations             = 7\nNumber of equality constraint evaluations            = 7\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 7\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 6\nTotal seconds in IPOPT                               = 0.209\n\nEXIT: Optimal Solution Found.\nOriginal objective: 6.232458632437464\n\n\nNow change the penalty coefficient and solve again:\n\nset_parameter!(c_param, θ, [200.0, 1.0])  # Double the penalty coefficient\nresult2 = ipopt(m_param)\nprintln(\"Modified penalty objective: $(result2.objective)\")\n\nThis is Ipopt version 3.14.19, running with linear solver MUMPS 5.8.2.\n\nNumber of nonzeros in equality constraint Jacobian...:       24\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       75\n\nTotal number of variables............................:       10\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        8\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.0898000e+03 2.48e+01 2.70e+01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n   1  2.1810502e+03 1.49e+01 8.27e+01  -1.0 2.20e+00    -  1.00e+00 1.00e+00f  1\n   2  6.5137192e+02 4.27e+00 1.36e+02  -1.0 1.43e+00    -  1.00e+00 1.00e+00f  1\n   3  2.4064340e+01 3.08e-01 2.18e+01  -1.0 5.62e-01    -  1.00e+00 1.00e+00f  1\n   4  8.6476680e+00 1.72e-02 8.45e-01  -1.0 2.10e-01    -  1.00e+00 1.00e+00f  1\n   5  8.6474398e+00 1.15e-05 8.07e-04  -1.7 3.39e-03    -  1.00e+00 1.00e+00h  1\n   6  8.6474398e+00 8.42e-12 7.91e-10  -5.7 2.03e-06    -  1.00e+00 1.00e+00h  1\n\nNumber of Iterations....: 6\n\n                                   (scaled)                 (unscaled)\nObjective...............:   5.4592422674820063e-01    8.6474397516914987e+00\nDual infeasibility......:   7.9051456536755353e-10    1.2521750715422049e-08\nConstraint violation....:   8.4190432403374871e-12    8.4190432403374871e-12\nVariable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   7.9051456536755353e-10    1.2521750715422049e-08\n\n\nNumber of objective function evaluations             = 7\nNumber of objective gradient evaluations             = 7\nNumber of equality constraint evaluations            = 7\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 7\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 6\nTotal seconds in IPOPT                               = 0.001\n\nEXIT: Optimal Solution Found.\nModified penalty objective: 8.647439751691499\n\n\nTry a different offset parameter:\n\nset_parameter!(c_param, θ, [200.0, 0.5])  # Change the offset in the objective\nresult3 = ipopt(m_param)\nprintln(\"Modified offset objective: $(result3.objective)\")\n\nThis is Ipopt version 3.14.19, running with linear solver MUMPS 5.8.2.\n\nNumber of nonzeros in equality constraint Jacobian...:       24\nNumber of nonzeros in inequality constraint Jacobian.:        0\nNumber of nonzeros in Lagrangian Hessian.............:       75\n\nTotal number of variables............................:       10\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        8\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  4.0810500e+03 2.48e+01 2.69e+01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n   1  2.1767809e+03 1.49e+01 8.26e+01  -1.0 2.20e+00    -  1.00e+00 1.00e+00f  1\n   2  6.5050886e+02 4.27e+00 1.36e+02  -1.0 1.43e+00    -  1.00e+00 1.00e+00f  1\n   3  2.4276149e+01 3.07e-01 2.18e+01  -1.0 5.61e-01    -  1.00e+00 1.00e+00f  1\n   4  8.8465512e+00 1.72e-02 8.43e-01  -1.0 2.09e-01    -  1.00e+00 1.00e+00f  1\n   5  8.8451636e+00 1.15e-05 8.04e-04  -1.7 3.40e-03    -  1.00e+00 1.00e+00h  1\n   6  8.8451630e+00 8.47e-12 7.88e-10  -5.7 2.05e-06    -  1.00e+00 1.00e+00h  1\n\nNumber of Iterations....: 6\n\n                                   (scaled)                 (unscaled)\nObjective...............:   5.5805444714793528e-01    8.8451629872947741e+00\nDual infeasibility......:   7.8812124187921384e-10    1.2491721683785540e-08\nConstraint violation....:   8.4678930534209940e-12    8.4678930534209940e-12\nVariable bound violation:   0.0000000000000000e+00    0.0000000000000000e+00\nComplementarity.........:   0.0000000000000000e+00    0.0000000000000000e+00\nOverall NLP error.......:   7.8812124187921384e-10    1.2491721683785540e-08\n\n\nNumber of objective function evaluations             = 7\nNumber of objective gradient evaluations             = 7\nNumber of equality constraint evaluations            = 7\nNumber of inequality constraint evaluations          = 0\nNumber of equality constraint Jacobian evaluations   = 7\nNumber of inequality constraint Jacobian evaluations = 0\nNumber of Lagrangian Hessian evaluations             = 6\nTotal seconds in IPOPT                               = 0.001\n\nEXIT: Optimal Solution Found.\nModified offset objective: 8.845162987294774\n\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"opf/#opf","page":"Example: Optimal Power Flow","title":"Example: Optimal Power Flow","text":"function parse_ac_power_data(filename)\n    data = PowerModels.parse_file(filename)\n    PowerModels.standardize_cost_terms!(data, order = 2)\n    PowerModels.calc_thermal_limits!(data)\n    ref = PowerModels.build_ref(data)[:it][:pm][:nw][0]\n\n    arcdict = Dict(a => k for (k, a) in enumerate(ref[:arcs]))\n    busdict = Dict(k => i for (i, (k, v)) in enumerate(ref[:bus]))\n    gendict = Dict(k => i for (i, (k, v)) in enumerate(ref[:gen]))\n    branchdict = Dict(k => i for (i, (k, v)) in enumerate(ref[:branch]))\n\n    return (\n        bus = [\n            begin\n                bus_loads = [ref[:load][l] for l in ref[:bus_loads][k]]\n                bus_shunts = [ref[:shunt][s] for s in ref[:bus_shunts][k]]\n                pd = sum(load[\"pd\"] for load in bus_loads; init = 0.0)\n                gs = sum(shunt[\"gs\"] for shunt in bus_shunts; init = 0.0)\n                qd = sum(load[\"qd\"] for load in bus_loads; init = 0.0)\n                bs = sum(shunt[\"bs\"] for shunt in bus_shunts; init = 0.0)\n                (i = busdict[k], pd = pd, gs = gs, qd = qd, bs = bs)\n            end for (k, v) in ref[:bus]\n        ],\n        gen = [\n            (\n                i = gendict[k],\n                cost1 = v[\"cost\"][1],\n                cost2 = v[\"cost\"][2],\n                cost3 = v[\"cost\"][3],\n                bus = busdict[v[\"gen_bus\"]],\n            ) for (k, v) in ref[:gen]\n        ],\n        arc = [\n            (i = k, rate_a = ref[:branch][l][\"rate_a\"], bus = busdict[i]) for\n            (k, (l, i, j)) in enumerate(ref[:arcs])\n        ],\n        branch = [\n            begin\n                f_idx = arcdict[i, branch[\"f_bus\"], branch[\"t_bus\"]]\n                t_idx = arcdict[i, branch[\"t_bus\"], branch[\"f_bus\"]]\n                g, b = PowerModels.calc_branch_y(branch)\n                tr, ti = PowerModels.calc_branch_t(branch)\n                ttm = tr^2 + ti^2\n                g_fr = branch[\"g_fr\"]\n                b_fr = branch[\"b_fr\"]\n                g_to = branch[\"g_to\"]\n                b_to = branch[\"b_to\"]\n                c1 = (-g * tr - b * ti) / ttm\n                c2 = (-b * tr + g * ti) / ttm\n                c3 = (-g * tr + b * ti) / ttm\n                c4 = (-b * tr - g * ti) / ttm\n                c5 = (g + g_fr) / ttm\n                c6 = (b + b_fr) / ttm\n                c7 = (g + g_to)\n                c8 = (b + b_to)\n                (\n                    i = branchdict[i],\n                    j = 1,\n                    f_idx = f_idx,\n                    t_idx = t_idx,\n                    f_bus = busdict[branch[\"f_bus\"]],\n                    t_bus = busdict[branch[\"t_bus\"]],\n                    c1 = c1,\n                    c2 = c2,\n                    c3 = c3,\n                    c4 = c4,\n                    c5 = c5,\n                    c6 = c6,\n                    c7 = c7,\n                    c8 = c8,\n                    rate_a_sq = branch[\"rate_a\"]^2,\n                )\n            end for (i, branch) in ref[:branch]\n        ],\n        ref_buses = [busdict[i] for (i, k) in ref[:ref_buses]],\n        vmax = [v[\"vmax\"] for (k, v) in ref[:bus]],\n        vmin = [v[\"vmin\"] for (k, v) in ref[:bus]],\n        pmax = [v[\"pmax\"] for (k, v) in ref[:gen]],\n        pmin = [v[\"pmin\"] for (k, v) in ref[:gen]],\n        qmax = [v[\"qmax\"] for (k, v) in ref[:gen]],\n        qmin = [v[\"qmin\"] for (k, v) in ref[:gen]],\n        rate_a = [ref[:branch][l][\"rate_a\"] for (k, (l, i, j)) in enumerate(ref[:arcs])],\n        angmax = [b[\"angmax\"] for (i, b) in ref[:branch]],\n        angmin = [b[\"angmin\"] for (i, b) in ref[:branch]],\n    )\nend\n\nconvert_data(data::N, backend) where {names,N<:NamedTuple{names}} =\n    NamedTuple{names}(ExaModels.convert_array(d, backend) for d in data)\n\nparse_ac_power_data(filename, backend) =\n    convert_data(parse_ac_power_data(filename), backend)\n\nfunction ac_power_model(filename; backend = nothing, T = Float64)\n\n    data = parse_ac_power_data(filename, backend)\n\n    w = ExaCore(T; backend = backend)\n\n    va = variable(w, length(data.bus);)\n\n    vm = variable(\n        w,\n        length(data.bus);\n        start = fill!(similar(data.bus, Float64), 1.0),\n        lvar = data.vmin,\n        uvar = data.vmax,\n    )\n    pg = variable(w, length(data.gen); lvar = data.pmin, uvar = data.pmax)\n\n    qg = variable(w, length(data.gen); lvar = data.qmin, uvar = data.qmax)\n\n    p = variable(w, length(data.arc); lvar = -data.rate_a, uvar = data.rate_a)\n\n    q = variable(w, length(data.arc); lvar = -data.rate_a, uvar = data.rate_a)\n\n    o = objective(w, g.cost1 * pg[g.i]^2 + g.cost2 * pg[g.i] + g.cost3 for g in data.gen)\n\n    c1 = constraint(w, va[i] for i in data.ref_buses)\n\n    c2 = constraint(\n        w,\n        p[b.f_idx] - b.c5 * vm[b.f_bus]^2 -\n        b.c3 * (vm[b.f_bus] * vm[b.t_bus] * cos(va[b.f_bus] - va[b.t_bus])) -\n        b.c4 * (vm[b.f_bus] * vm[b.t_bus] * sin(va[b.f_bus] - va[b.t_bus])) for\n        b in data.branch\n    )\n\n    c3 = constraint(\n        w,\n        q[b.f_idx] +\n        b.c6 * vm[b.f_bus]^2 +\n        b.c4 * (vm[b.f_bus] * vm[b.t_bus] * cos(va[b.f_bus] - va[b.t_bus])) -\n        b.c3 * (vm[b.f_bus] * vm[b.t_bus] * sin(va[b.f_bus] - va[b.t_bus])) for\n        b in data.branch\n    )\n\n    c4 = constraint(\n        w,\n        p[b.t_idx] - b.c7 * vm[b.t_bus]^2 -\n        b.c1 * (vm[b.t_bus] * vm[b.f_bus] * cos(va[b.t_bus] - va[b.f_bus])) -\n        b.c2 * (vm[b.t_bus] * vm[b.f_bus] * sin(va[b.t_bus] - va[b.f_bus])) for\n        b in data.branch\n    )\n\n    c5 = constraint(\n        w,\n        q[b.t_idx] +\n        b.c8 * vm[b.t_bus]^2 +\n        b.c2 * (vm[b.t_bus] * vm[b.f_bus] * cos(va[b.t_bus] - va[b.f_bus])) -\n        b.c1 * (vm[b.t_bus] * vm[b.f_bus] * sin(va[b.t_bus] - va[b.f_bus])) for\n        b in data.branch\n    )\n\n    c6 = constraint(\n        w,\n        va[b.f_bus] - va[b.t_bus] for b in data.branch;\n        lcon = data.angmin,\n        ucon = data.angmax,\n    )\n    c7 = constraint(\n        w,\n        p[b.f_idx]^2 + q[b.f_idx]^2 - b.rate_a_sq for b in data.branch;\n        lcon = fill!(similar(data.branch, Float64, length(data.branch)), -Inf),\n    )\n    c8 = constraint(\n        w,\n        p[b.t_idx]^2 + q[b.t_idx]^2 - b.rate_a_sq for b in data.branch;\n        lcon = fill!(similar(data.branch, Float64, length(data.branch)), -Inf),\n    )\n\n    c9 = constraint(w, b.pd + b.gs * vm[b.i]^2 for b in data.bus)\n\n    c10 = constraint(w, b.qd - b.bs * vm[b.i]^2 for b in data.bus)\n\n    c11 = constraint!(w, c9, a.bus => p[a.i] for a in data.arc)\n    c12 = constraint!(w, c10, a.bus => q[a.i] for a in data.arc)\n\n    c13 = constraint!(w, c9, g.bus => -pg[g.i] for g in data.gen)\n    c14 = constraint!(w, c10, g.bus => -qg[g.i] for g in data.gen)\n\n    return ExaModel(w)\n\nend\n\nac_power_model (generic function with 1 method)\n\nWe first download the case file.\n\nusing Downloads\n\ncase = tempname() * \".m\"\n\nDownloads.download(\n    \"https://raw.githubusercontent.com/power-grid-lib/pglib-opf/dc6be4b2f85ca0e776952ec22cbd4c22396ea5a3/pglib_opf_case3_lmbd.m\",\n    case,\n)\n\n\"/tmp/jl_ClaJY3iFWR.m\"\n\nThen, we can model/sovle the problem.\n\nusing PowerModels, ExaModels, NLPModelsIpopt\n\nm = ac_power_model(case)\nipopt(m)\n\n\"Execution stats: first-order stationary\"\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"performance/#Performance-Tips","page":"Performance Tips","title":"Performance Tips","text":"","category":"section"},{"location":"performance/#Use-a-function-to-create-a-model","page":"Performance Tips","title":"Use a function to create a model","text":"It is always better to use functions to create ExaModels. This in this way, the functions used for specifing objective/constraint functions are not recreated over all over, and thus, we can take advantage of the previously compiled model creation code. Let's consider the following example.\n\nusing ExaModels\n\nt = @elapsed begin\n    c = ExaCore()\n    N = 10\n    x = variable(c, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N))\n    objective(c, 100 * (x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2 for i = 2:N)\n    constraint(\n        c,\n        3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] - x[i]exp(x[i] - x[i+1]) - 3 for i = 1:(N-2)\n    )\n    m = ExaModel(c)\nend\n\nprintln(\"$t seconds elapsed\")\n\n0.087744785 seconds elapsed\n\n\nEven at the second call,\n\nt = @elapsed begin\n    c = ExaCore()\n    N = 10\n    x = variable(c, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N))\n    objective(c, 100 * (x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2 for i = 2:N)\n    constraint(\n        c,\n        3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] - x[i]exp(x[i] - x[i+1]) - 3 for i = 1:(N-2)\n    )\n    m = ExaModel(c)\nend\n\nprintln(\"$t seconds elapsed\")\n\n0.078688028 seconds elapsed\n\n\nthe model creation time can be slightly reduced but the compilation time is still quite significant.\n\nBut instead, if you create a function, we can significantly reduce the model creation time.\n\nfunction luksan_vlcek_model(N)\n    c = ExaCore()\n    x = variable(c, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N))\n    objective(c, 100 * (x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2 for i = 2:N)\n    constraint(\n        c,\n        3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n        x[i]exp(x[i] - x[i+1]) - 3 for i = 1:(N-2)\n    )\n    m = ExaModel(c)\nend\n\nt = @elapsed luksan_vlcek_model(N)\nprintln(\"$t seconds elapsed\")\n\n0.078021606 seconds elapsed\n\n\nt = @elapsed luksan_vlcek_model(N)\nprintln(\"$t seconds elapsed\")\n\n8.1e-5 seconds elapsed\n\n\nSo, the model creation time can be essentially nothing. Thus, if you care about the model creation time, always make sure to write a function for creating the model, and do not directly create a model from the REPL.","category":"section"},{"location":"performance/#Make-sure-your-array's-eltype-is-concrete","page":"Performance Tips","title":"Make sure your array's eltype is concrete","text":"In order for ExaModels to run for loops over the array you provided without any overhead caused by type inference, the eltype of the data array should always be a concrete type. Furthermore, this is required if you want to run ExaModels on GPU accelerators.\n\nLet's take an example.\n\nusing ExaModels\n\nN = 1000\n\nfunction luksan_vlcek_model_concrete(N)\n    c = ExaCore()\n\n    arr1 = Array(2:N)\n    arr2 = Array(1:(N-2))\n\n    x = variable(c, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N))\n    objective(c, 100 * (x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2 for i in arr1)\n    constraint(\n        c,\n        3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n        x[i]exp(x[i] - x[i+1]) - 3 for i in arr2\n    )\n    m = ExaModel(c)\nend\n\nfunction luksan_vlcek_model_non_concrete(N)\n    c = ExaCore()\n\n    arr1 = Array{Any}(2:N)\n    arr2 = Array{Any}(1:(N-2))\n\n    x = variable(c, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N))\n    objective(c, 100 * (x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2 for i in arr1)\n    constraint(\n        c,\n        3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n        x[i]exp(x[i] - x[i+1]) - 3 for i in arr2\n    )\n    m = ExaModel(c)\nend\n\nluksan_vlcek_model_non_concrete (generic function with 1 method)\n\nHere, observe that\n\nisconcretetype(eltype(Array(2:N)))\n\ntrue\n\nisconcretetype(eltype(Array{Any}(2:N)))\n\nfalse\n\nAs you can see, the first array type has concrete eltypes, whereas the second array type has non concrete eltypes. Due to this, the array stored in the model created by luksan_vlcek_model_non_concrete will have non-concrete eltypes.\n\nNow let's compare the performance. We will use the following benchmark function here.\n\nusing NLPModels\n\nfunction benchmark_callbacks(m; N = 100)\n    nvar = m.meta.nvar\n    ncon = m.meta.ncon\n    nnzj = m.meta.nnzj\n    nnzh = m.meta.nnzh\n\n    x = copy(m.meta.x0)\n    y = similar(m.meta.x0, ncon)\n    c = similar(m.meta.x0, ncon)\n    g = similar(m.meta.x0, nvar)\n    jac = similar(m.meta.x0, nnzj)\n    hess = similar(m.meta.x0, nnzh)\n    jrows = similar(m.meta.x0, Int, nnzj)\n    jcols = similar(m.meta.x0, Int, nnzj)\n    hrows = similar(m.meta.x0, Int, nnzh)\n    hcols = similar(m.meta.x0, Int, nnzh)\n\n    GC.enable(false)\n\n    NLPModels.obj(m, x) # to compile\n\n    tobj = (1 / N) * @elapsed for t = 1:N\n        NLPModels.obj(m, x)\n    end\n\n    NLPModels.cons!(m, x, c) # to compile\n    tcon = (1 / N) * @elapsed for t = 1:N\n        NLPModels.cons!(m, x, c)\n    end\n\n    NLPModels.grad!(m, x, g) # to compile\n    tgrad = (1 / N) * @elapsed for t = 1:N\n        NLPModels.grad!(m, x, g)\n    end\n\n    NLPModels.jac_coord!(m, x, jac) # to compile\n    tjac = (1 / N) * @elapsed for t = 1:N\n        NLPModels.jac_coord!(m, x, jac)\n    end\n\n    NLPModels.hess_coord!(m, x, y, hess) # to compile\n    thess = (1 / N) * @elapsed for t = 1:N\n        NLPModels.hess_coord!(m, x, y, hess)\n    end\n\n    NLPModels.jac_structure!(m, jrows, jcols) # to compile\n    tjacs = (1 / N) * @elapsed for t = 1:N\n        NLPModels.jac_structure!(m, jrows, jcols)\n    end\n\n    NLPModels.hess_structure!(m, hrows, hcols) # to compile\n    thesss = (1 / N) * @elapsed for t = 1:N\n        NLPModels.hess_structure!(m, hrows, hcols)\n    end\n\n    GC.enable(true)\n\n    return (\n        tobj = tobj,\n        tcon = tcon,\n        tgrad = tgrad,\n        tjac = tjac,\n        thess = thess,\n        tjacs = tjacs,\n        thesss = thesss,\n    )\nend\n\nbenchmark_callbacks (generic function with 1 method)\n\nThe performance comparison is here:\n\nm1 = luksan_vlcek_model_concrete(N)\nm2 = luksan_vlcek_model_non_concrete(N)\n\nbenchmark_callbacks(m1)\n\n(tobj = 7.6449e-7, tcon = 1.440792e-5, tgrad = 2.60438e-6, tjac = 2.6169140000000003e-5, thess = 0.00031181455, tjacs = 7.81822e-6, thesss = 1.416991e-5)\n\nbenchmark_callbacks(m2)\n\n(tobj = 3.060812e-5, tcon = 6.648466e-5, tgrad = 3.825421e-5, tjac = 0.00018443447000000002, thess = 0.00074640309, tjacs = 0.00023959382000000002, thesss = 0.00039279077)\n\nAs can be seen here, having concrete eltype dramatically improves the performance. This is because when all the data arrays' eltypes are concrete, the AD evaluations can be performed without any type inferernce, and this should be as fast as highly optimized C/C++/Fortran code.\n\nWhen you're using GPU accelerators, the eltype of the array should always be concrete. In fact, non-concrete etlype will already cause an error when creating the array. For example,\n\nusing CUDA\n\ntry\n    arr1 = CuArray(Array{Any}(2:N))\ncatch e\n    showerror(stdout, e)\nend\n\nCuArray only supports element types that are allocated inline.\nAny is not allocated inline\n\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"quad/#quad","page":"Example: Quadrotor","title":"Example: Quadrotor","text":"function quadrotor_model(N = 3; backend = nothing)\n\n    n = 9\n    p = 4\n    d(i, j, N) =\n        (j == 1 ? 1 * sin(2 * pi / N * i) : 0.0) +\n        (j == 3 ? 2 * sin(4 * pi / N * i) : 0.0) +\n        (j == 5 ? 2 * i / N : 0.0)\n    dt = 1/N\n    R = fill(1 / 10, 4)\n    Q = [1, 0, 1, 0, 1, 0, 1, 1, 1]\n    Qf = [1, 0, 1, 0, 1, 0, 1, 1, 1] / dt\n\n    x0s = [(i, 0.0) for i = 1:n]\n    itr0 = [(i, j, R[j]) for (i, j) in Base.product(1:N, 1:p)]\n    itr1 = [(i, j, Q[j], d(i, j, N)) for (i, j) in Base.product(1:N, 1:n)]\n    itr2 = [(j, Qf[j], d(N + 1, j, N)) for j = 1:n]\n\n    c = ExaCore(; backend = backend)\n\n    x = variable(c, 1:(N+1), 1:n)\n    u = variable(c, 1:N, 1:p)\n\n    constraint(c, x[1, i] - x0 for (i, x0) in x0s)\n    constraint(c, -x[i+1, 1] + x[i, 1] + (x[i, 2]) * dt for i = 1:N)\n    constraint(\n        c,\n        -x[i+1, 2] +\n        x[i, 2] +\n        (\n            u[i, 1] * cos(x[i, 7]) * sin(x[i, 8]) * cos(x[i, 9]) +\n            u[i, 1] * sin(x[i, 7]) * sin(x[i, 9])\n        ) * dt for i = 1:N\n    )\n    constraint(c, -x[i+1, 3] + x[i, 3] + (x[i, 4]) * dt for i = 1:N)\n    constraint(\n        c,\n        -x[i+1, 4] +\n        x[i, 4] +\n        (\n            u[i, 1] * cos(x[i, 7]) * sin(x[i, 8]) * sin(x[i, 9]) -\n            u[i, 1] * sin(x[i, 7]) * cos(x[i, 9])\n        ) * dt for i = 1:N\n    )\n    constraint(c, -x[i+1, 5] + x[i, 5] + (x[i, 6]) * dt for i = 1:N)\n    constraint(\n        c,\n        -x[i+1, 6] + x[i, 6] + (u[i, 1] * cos(x[i, 7]) * cos(x[i, 8]) - 9.8) * dt for\n        i = 1:N\n    )\n    constraint(\n        c,\n        -x[i+1, 7] +\n        x[i, 7] +\n        (u[i, 2] * cos(x[i, 7]) / cos(x[i, 8]) + u[i, 3] * sin(x[i, 7]) / cos(x[i, 8])) * dt\n        for i = 1:N\n    )\n    constraint(\n        c,\n        -x[i+1, 8] + x[i, 8] + (-u[i, 2] * sin(x[i, 7]) + u[i, 3] * cos(x[i, 7])) * dt for\n        i = 1:N\n    )\n    constraint(\n        c,\n        -x[i+1, 9] +\n        x[i, 9] +\n        (\n            u[i, 2] * cos(x[i, 7]) * tan(x[i, 8]) +\n            u[i, 3] * sin(x[i, 7]) * tan(x[i, 8]) +\n            u[i, 4]\n        ) * dt for i = 1:N\n    )\n\n    objective(c, 0.5 * R * (u[i, j]^2) for (i, j, R) in itr0)\n    objective(c, 0.5 * Q * (x[i, j] - d)^2 for (i, j, Q, d) in itr1)\n    objective(c, 0.5 * Qf * (x[N+1, j] - d)^2 for (j, Qf, d) in itr2)\n\n    m = ExaModel(c)\n\nend\n\nquadrotor_model (generic function with 2 methods)\n\nusing ExaModels, NLPModelsIpopt\n\nm = quadrotor_model(100)\nresult = ipopt(m)\n\n\"Execution stats: first-order stationary\"\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"two_stage/#two_stage","page":"Two-Stage Optimization","title":"Two-Stage Optimization","text":"ExaModels supports two-stage optimization problems through the TwoStageExaModel. This feature enables efficient modeling of optimization problems where decisions are made in two stages:\n\nDesign (first-stage) variables: Decisions made before uncertainty is revealed, shared across all scenarios\nRecourse (second-stage) variables: Scenario-specific decisions made after uncertainty is revealed\nScenarios: Each scenario has its own parameters θ that affect the objective and constraints\n\nThe key advantage of TwoStageExaModel is that all scenarios share one compiled expression pattern, achieving true SIMD parallelism on GPUs while maintaining the block-structured nature of the problem.","category":"section"},{"location":"two_stage/#Problem-Formulation","page":"Two-Stage Optimization","title":"Problem Formulation","text":"A typical two-stage program has the form:\n\nbeginaligned\nmin_d v_i quad  f(d) + sum_i=1^S w_i cdot g_i(d v_i theta_i) \ntextst quad  h_i(d v_i theta_i) = 0 quad i = 1 ldots S \n d in mathcalD quad v_i in mathcalV_i\nendaligned\n\nwhere:\n\nd\nare design (first-stage) variables\nv_i\nare recourse (second-stage) variables for scenario i\ntheta_i\nare the parameters for scenario i\nS\nis the number of scenarios\nw_i\nare scenario weights (e.g., probabilities)","category":"section"},{"location":"two_stage/#Building-a-Two-Stage-Model","page":"Two-Stage Optimization","title":"Building a Two-Stage Model","text":"Let's build a simple two-stage problem. Consider minimizing design costs plus expected recourse costs:\n\nbeginaligned\nmin_d v_i quad  d^2 + frac1S sum_i=1^S (v_i - theta_i)^2 \ntextst quad  v_i geq d quad i = 1 ldots S\nendaligned\n\nHere, the design variable d sets a minimum level that all recourse variables must meet, while each scenario tries to match its target theta_i.\n\nusing ExaModels, NLPModelsIpopt\n\nDefine the problem dimensions and scenario parameters:\n\nns = 3   ## number of scenarios\nnv = 1   ## recourse variables per scenario\nnd = 1   ## design variables\nθ_sets = [[2.0], [4.0], [6.0]]  ## θ₁=2, θ₂=4, θ₃=6\nweight = 1.0 / ns\n\n0.3333333333333333\n\nBuild the model using the do-block syntax. The build function receives the ExaCore c, variable handles d and v, parameter handle θ, and dimension info ns, nv, nθ:\n\nmodel = TwoStageExaModel(nd, nv, ns, θ_sets) do c, d, v, θ, ns, nv, nθ\n    # Design objective: d²\n    objective(c, d[1]^2)\n    # Recourse objective: weight * Σᵢ (vᵢ - θᵢ)²\n    obj_data = [(i, i, (i - 1) * nθ) for i in 1:ns]\n    objective(c, weight * (v[v_idx] - θ[θ_off + 1])^2 for (i, v_idx, θ_off) in obj_data)\n    # Constraints: vᵢ ≥ d (i.e., vᵢ - d ≥ 0)\n    con_data = [(i, i) for i in 1:ns]\n    constraint(c, v[v_idx] - d[1] for (i, v_idx) in con_data; lcon = 0.0)\nend\n\nTwoStageExaModel{Float64, Vector{Float64}}\n  Scenarios: 3\n  Recourse vars per scenario: 1\n  Design vars (shared): 1\n  Constraints per scenario: 1\n  Total variables: 4\n  Total constraints: 3\n  Jacobian nnz per scenario: 2\n  Hessian nnz per scenario: 1\n\n\nThe TwoStageExaModel constructor takes:\n\nnd: number of design variables\nnv: number of recourse variables per scenario\nns: number of scenarios\nθ_sets: vector of parameter vectors, one per scenario\n\nThe build function receives:\n\nc: the ExaCore to build on\nd: variable handle for design variables (indices 1:nd)\nv: variable handle for ALL recourse variables (use global indexing)\nθ: parameter handle for ALL parameters (use global indexing)\nns, nv, nθ: dimensions for building iteration data","category":"section"},{"location":"two_stage/#Variable-Layout","page":"Two-Stage Optimization","title":"Variable Layout","text":"Variables are stored in a global vector with the layout: [v₁; v₂; ...; vₛ; d]\n\nRecourse variables for all scenarios come first\nDesign variables are at the end\n\nUse the helper functions to get index ranges:\n\nprintln(\"Recourse var indices for scenario 1: \", ExaModels.recourse_var_indices(model, 1))\nprintln(\"Recourse var indices for scenario 2: \", ExaModels.recourse_var_indices(model, 2))\nprintln(\"Recourse var indices for scenario 3: \", ExaModels.recourse_var_indices(model, 3))\nprintln(\"Design var indices: \", ExaModels.design_var_indices(model))\nprintln(\"Total variables: \", ExaModels.total_vars(model))\nprintln(\"Total constraints: \", ExaModels.total_cons(model))\n\nRecourse var indices for scenario 1: 1:1\nRecourse var indices for scenario 2: 2:2\nRecourse var indices for scenario 3: 3:3\nDesign var indices: 4:4\nTotal variables: 4\nTotal constraints: 3\n","category":"section"},{"location":"two_stage/#Solving-and-Extracting-Solutions","page":"Two-Stage Optimization","title":"Solving and Extracting Solutions","text":"Solve the model using Ipopt:\n\nresult = ipopt(model.model; print_level = 0)\nprintln(\"\\nSolution status: \", result.status)\nprintln(\"Optimal objective: \", round(result.objective, digits = 4))\n\n\nSolution status: first_order\nOptimal objective: 10.6667\n\n\nExtract solutions using index ranges:\n\nx_sol = result.solution\nd_sol = x_sol[ExaModels.design_var_indices(model)]\nprintln(\"\\nDesign variable d* = \", round(d_sol[1], digits = 4))\n\nfor i in 1:ns\n    v_sol = x_sol[ExaModels.recourse_var_indices(model, i)]\n    println(\"Recourse variable v$(i)* = \", round(v_sol[1], digits = 4))\nend\n\n\nDesign variable d* = 2.0\nRecourse variable v1* = 2.0\nRecourse variable v2* = 2.0\nRecourse variable v3* = 2.0\n\n\nFor this problem, the optimal solution is d* = θ̄/2 = 2, and all vᵢ* = d* = 2 (the constraint vᵢ ≥ d is binding for all scenarios since d* ≤ min(θᵢ)).","category":"section"},{"location":"two_stage/#Updating-Parameters","page":"Two-Stage Optimization","title":"Updating Parameters","text":"One powerful feature is the ability to update scenario parameters and re-solve without rebuilding the model. This is useful for:\n\nStochastic programming with sample average approximation (SAA)\nSensitivity analysis\nOnline optimization with changing uncertainty\n\nUpdate parameters for a single scenario:\n\nExaModels.set_scenario_parameters!(model, 1, [10.0])  ## Change θ₁ from 2.0 to 10.0\n\nOr update all scenarios at once:\n\nnew_θ_sets = [[10.0], [12.0], [14.0]]\nExaModels.set_all_scenario_parameters!(model, new_θ_sets)\n\nRe-solve with new parameters:\n\nresult2 = ipopt(model.model; print_level = 0)\nprintln(\"\\nAfter parameter update:\")\nprintln(\"New optimal objective: \", round(result2.objective, digits = 4))\nx_sol2 = result2.solution\nd_sol2 = x_sol2[ExaModels.design_var_indices(model)]\nprintln(\"New design variable d* = \", round(d_sol2[1], digits = 4))\n\n\nAfter parameter update:\nNew optimal objective: 74.6667\nNew design variable d* = 6.0\n","category":"section"},{"location":"two_stage/#Variable-Bounds-and-Initial-Values","page":"Two-Stage Optimization","title":"Variable Bounds and Initial Values","text":"You can specify bounds and initial values for both design and recourse variables:\n\nmodel = TwoStageExaModel(nd, nv, ns, θ_sets;\n    d_start = 1.0,      # initial value for design variables\n    d_lvar = 0.0,       # lower bound for design variables\n    d_uvar = 10.0,      # upper bound for design variables\n    v_start = 0.5,      # initial value for recourse variables\n    v_lvar = 0.0,       # lower bound for recourse variables\n    v_uvar = Inf        # upper bound for recourse variables\n) do c, d, v, θ, ns, nv, nθ\n    # ... build model\nend","category":"section"},{"location":"two_stage/#Building-Iteration-Data","page":"Two-Stage Optimization","title":"Building Iteration Data","text":"The key to using TwoStageExaModel is correctly building iteration data for objectives and constraints. Since all scenarios share one compiled pattern, you must:\n\nCreate tuples that map scenario indices to global variable/parameter indices\nUse generators that iterate over all scenarios\n\nHere's a more complex example with multiple recourse variables per scenario:\n\nns2, nv2, nd2 = 2, 2, 2\nθ_sets2 = [[1.0, 3.0], [2.0, 2.0]]  ## 2 params per scenario\n\nmodel2 = TwoStageExaModel(nd2, nv2, ns2, θ_sets2) do c, d, v, θ, ns, nv, nθ\n    objective(c, d[1]^2 + d[2]^2)\n    # Recourse objective: Σᵢ Σⱼ (vᵢⱼ - θᵢⱼ)²\n    obj_data = [(i, j, (i - 1) * nv + j, (i - 1) * nθ + j) for i in 1:ns for j in 1:nv]\n    objective(c, (v[v_idx] - θ[θ_idx])^2 for (i, j, v_idx, θ_idx) in obj_data)\n    # Coupling constraint: v_{i,1} + v_{i,2} = d₁ + d₂ for each scenario\n    con_data = [(i, (i - 1) * nv + 1, (i - 1) * nv + 2) for i in 1:ns]\n    constraint(c, v[v1] + v[v2] - d[1] - d[2] for (i, v1, v2) in con_data;\n               lcon = 0.0, ucon = 0.0)\nend\n\nresult3 = ipopt(model2.model; print_level = 0)\nprintln(\"\\nMulti-variable example:\")\nprintln(\"Status: \", result3.status)\nprintln(\"Optimal objective: \", round(result3.objective, digits = 4))\n\n\nMulti-variable example:\nStatus: first_order\nOptimal objective: 5.3333\n","category":"section"},{"location":"two_stage/#Accessing-the-Underlying-Model","page":"Two-Stage Optimization","title":"Accessing the Underlying Model","text":"For advanced use cases, you can access the underlying ExaModel:\n\ninner_model = ExaModels.get_model(model)\nprintln(\"\\nUnderlying model type: \", typeof(inner_model))\n\n\nUnderlying model type: ExaModel{Float64, Vector{Float64}, Nothing, ExaModels.Objective{ExaModels.Objective{ExaModels.ObjectiveNull, ExaModels.SIMDFunction{ExaModels.Node1{typeof(abs2), ExaModels.Var{Int64}}, ExaModels.Compressor{Tuple{Int64}}, ExaModels.Compressor{Tuple{Int64}}}, UnitRange{Int64}}, ExaModels.SIMDFunction{ExaModels.Node2{typeof(*), Float64, ExaModels.Node1{typeof(abs2), ExaModels.Node2{typeof(-), ExaModels.Var{ExaModels.ParIndexed{ExaModels.ParSource, 2}}, ExaModels.ParameterNode{ExaModels.Node2{typeof(+), ExaModels.ParIndexed{ExaModels.ParSource, 3}, Int64}}}}}, ExaModels.Compressor{Tuple{Int64}}, ExaModels.Compressor{Tuple{Int64}}}, Vector{Tuple{Int64, Int64, Int64}}}, ExaModels.Constraint{ExaModels.ConstraintNull, ExaModels.SIMDFunction{ExaModels.Node2{typeof(-), ExaModels.Var{ExaModels.ParIndexed{ExaModels.ParSource, 2}}, ExaModels.Var{Int64}}, ExaModels.Compressor{Tuple{Int64, Int64}}, ExaModels.Compressor{Tuple{}}}, Vector{Tuple{Int64, Int64}}, Int64}}\n\n\nThis allows you to use any NLPModels-compatible solver or perform custom operations on the model.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"guide/#guide","page":"Getting Started","title":"Getting Started","text":"ExaModels can create nonlinear prgogramming models and allows solving the created models using NLP solvers (in particular, those that are interfaced with NLPModels, such as NLPModelsIpopt and MadNLP. This documentation page will describe how to use ExaModels to model and solve nonlinear optimization problems.\n\nWe will first consider the following simple nonlinear program [3]:\n\nbeginaligned\nmin_x_i_i=0^N sum_i=2^N  100(x_i-1^2-x_i)^2+(x_i-1-1)^2\ntextst   3x_i+1^3+2x_i+2-5+sin(x_i+1-x_i+2)sin(x_i+1+x_i+2)+4x_i+1-x_i e^x_i-x_i+1-3 = 0\nendaligned\n\nWe will follow the following Steps to create the model/solve this optimization problem.\n\nStep 0: import ExaModels.jl\nStep 1: create a ExaCore object, wherein we can progressively build an optimization model.\nStep 2: create optimization variables with variable, while attaching it to previously created ExaCore.\nStep 3 (interchangable with Step 3): create objective function with objective, while attaching it to previously created ExaCore.\nStep 4 (interchangable with Step 2): create constraints with constraint, while attaching it to previously created ExaCore.\nStep 5: create an ExaModel based on the ExaCore.\n\nNow, let's jump right in. We import ExaModels via (Step 0):\n\nusing ExaModels\n\nNow, all the functions that are necessary for creating model are imported to into Main.","category":"section"},{"location":"guide/#ExaCore","page":"Getting Started","title":"ExaCore","text":"An ExaCore object can be created simply by (Step 1):\n\nc = ExaCore()\n\nAn ExaCore\n\n  Float type: ...................... Float64\n  Array type: ...................... Vector{Float64}\n  Backend: ......................... Nothing\n\n  number of objective patterns: .... 0\n  number of constraint patterns: ... 0\n\n\nThis is where our optimziation model information will be progressively stored. This object is not yet an NLPModel, but it will essentially store all the necessary information.","category":"section"},{"location":"guide/#Variables","page":"Getting Started","title":"Variables","text":"Now, let's create the optimziation variables. From the problem definition, we can see that we will need N scalar variables. We will choose N=10, and create the variable xinmathbbR^N with the follwoing command:\n\nN = 10\nx = variable(c, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N))\n\nVariable\n\n  x ∈ R^{10}\n\n\nThis creates the variable x, which we will be able to refer to when we create constraints/objective constraints. Also, this modifies the information in the ExaCore object properly so that later an optimization model can be properly created with the necessary information. Observe that we have used the keyword argument start to specify the initial guess for the solution. The variable upper and lower bounds can be specified in a similar manner. For example, if we wanted to set the lower bound of the variable x to 0.0 and the upper bound to 10.0, we could do it as follows:\n\nx = variable(c, N; start = (mod(i, 2) == 1 ? -1.2 : 1.0 for i = 1:N), lvar = 0.0, uvar = 10.0)","category":"section"},{"location":"guide/#Objective","page":"Getting Started","title":"Objective","text":"The objective can be set as follows:\n\nobjective(c, 100 * (x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2 for i = 2:N)\n\nObjective\n\n  min (...) + ∑_{p ∈ P} f(x,θ,p)\n\n  where |P| = 9\n\n\nnote: Note\nNote that the terms here are summed, without explicitly using sum( ... ) syntax.","category":"section"},{"location":"guide/#Constraints","page":"Getting Started","title":"Constraints","text":"The constraints can be set as follows:\n\nconstraint(\n    c,\n    3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n    x[i]exp(x[i] - x[i+1]) - 3 for i = 1:(N-2)\n)\n\nConstraint\n\n  s.t. (...)\n       g♭ ≤ [g(x,θ,p)]_{p ∈ P} ≤ g♯\n\n  where |P| = 8\n\n\nNote that ExaModels always assume that the constraints are doubly-bounded inequalities. That is, the constraint above is treated as\n\n g^flat leq leftg^(m)(x q_j)right_jin J_m +sum_nin N_msum_kin K_nh^(n)(x s^(n)_k) leq g^sharp\n\nwhere g^\\flat and g^\\sharp are the lower and upper bounds of the constraint, respectively. In this case, both bounds are zero, i.e., g^\\flat = g^\\sharp = 0.\n\nYou can use the keyword arguments lcon and ucon to specify the lower and upper bounds of the constraints, respectively. For example, if we wanted to set the lower bound of the constraint to -1 and the upper bound to 1, we could do it as follows:\n\nconstraint(\n    c,\n    3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n    x[i]exp(x[i] - x[i+1]) - 3 for i = 1:(N-2);\n    lcon = -1.0, ucon = 1.0\n)\n\nConstraint\n\n  s.t. (...)\n       g♭ ≤ [g(x,θ,p)]_{p ∈ P} ≤ g♯\n\n  where |P| = 8\n\n\nIf you want to create a single-bounded constraint, you can set lcon to -Inf or ucon to Inf. For example, if we wanted to set the lower bound of the constraint to -1 and the upper bound to infinity, we could do it as follows:\n\nconstraint(\n    c,\n    3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n    x[i]exp(x[i] - x[i+1]) - 3 for i = 1:(N-2);\n    lcon = -1.0, ucon = Inf\n)\n\nConstraint\n\n  s.t. (...)\n       g♭ ≤ [g(x,θ,p)]_{p ∈ P} ≤ g♯\n\n  where |P| = 8\n","category":"section"},{"location":"guide/#Subexpressions","page":"Getting Started","title":"Subexpressions","text":"When complex expressions are reused across multiple objectives or constraints, you can define them as subexpressions using subexpr. This improves code readability and can help with derivative computation efficiency.\n\nSubexpressions are \"lifted\" to auxiliary variables with defining equality constraints. This means:\n\nEach subexpression creates new auxiliary variables\nEquality constraints define the relationship between the subexpression and its value\nDerivative code is generated once per subexpression pattern\n\nHere's a simple example:\n\nc2 = ExaCore()\ny = variable(c2, N; start = 1.0)\n\nVariable\n\n  x ∈ R^{10}\n\n\nDefine a subexpression for y[i]^2\n\ns = subexpr(c2, y[i]^2 for i in 1:N)\n\nSubexpression (lifted)\n\n  s ∈ R^{10}\n\n\nNow s[i] can be used in objectives and constraints\n\nobjective(c2, (s[i] - 1)^2 for i in 1:N)\nconstraint(c2, s[i] + s[i + 1] for i in 1:(N - 1); lcon = 0.0)\n\nConstraint\n\n  s.t. (...)\n       g♭ ≤ [g(x,θ,p)]_{p ∈ P} ≤ g♯\n\n  where |P| = 9\n\n\nMulti-dimensional subexpressions are also supported with automatic dimension inference:\n\ndx = subexpr(c, x[t, i] - x[t-1, i] for t in 1:T, i in 1:N)\n# dx[t, i] can now be used in constraints\nconstraint(c, dx[t, i] - something for t in 1:T, i in 1:N)\n\nFor a comprehensive example using subexpressions, see the Distillation Column example.","category":"section"},{"location":"guide/#ExaModel","page":"Getting Started","title":"ExaModel","text":"Finally, we are ready to create an ExaModel from the data we have collected in ExaCore. Since ExaCore includes all the necessary information, we can do this simply by:\n\nm = ExaModel(c)\n\nAn ExaModel{Float64, Vector{Float64}, ...}\n\n  Problem name: Generic\n   All variables: ████████████████████ 10     All constraints: ████████████████████ 24    \n            free: ████████████████████ 10                free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ███████⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 8     \n           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ███████⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 8     \n           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ███████⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 8     \n          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n            nnzh: (-210.91% sparsity)   171             linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n                                                    nonlinear: ████████████████████ 24    \n                                                         nnzj: ( 70.00% sparsity)   72    \n                                                     lin_nnzj: (------% sparsity)         \n                                                     nln_nnzj: ( 70.00% sparsity)   72    \n\n\n\nNow, we got an optimization model ready to be solved. This problem can be solved with for example, with the Ipopt solver, as follows.\n\nusing NLPModelsIpopt\nresult = ipopt(m)\n\n\"Execution stats: first-order stationary\"\n\nHere, result is an AbstractExecutionStats, which typically contains the solution information. We can check several information as follows.\n\nprintln(\"Status: $(result.status)\")\nprintln(\"Number of iterations: $(result.iter)\")\n\nStatus: first_order\nNumber of iterations: 8\n","category":"section"},{"location":"guide/#Solutions","page":"Getting Started","title":"Solutions","text":"The solution values for variable x can be inquired by:\n\nsol = solution(result, x)\n\n10-element view(::Vector{Float64}, 1:10) with eltype Float64:\n -0.9505563573613093\n  0.9139008176388945\n  0.9890905176644905\n  0.9985592422681151\n  0.9998087408802769\n  0.9999745932450962\n  0.9999966246997652\n  0.999999551252415\n  0.9999999449191489\n  0.9999999300651202\n\nThis will return the primal solution of the variable x as a vector. Dual solutions can be inquired similarly, by using the multipliers function.\n\nExaModels provide several APIs similar to this:\n\nsolution inquires the primal solution.\nmultipliers inquires the dual solution.\nmultipliers_L inquires the lower bound dual solution.\nmultipliers_U inquires the upper bound dual solution.\n\nThis concludes a short tutorial on how to use ExaModels to model and solve optimization problems. Want to learn more? Take a look at the following examples, which provide further tutorial on how to use ExaModels.jl. Each of the examples are designed to instruct a few additional techniques.\n\nExample: Quadrotor: modeling multiple types of objective values and constraints.\nExample: Distillation Column: using subexpressions and two-dimensional index sets.\nExample: Optimal Power Flow: handling complex data and using constraint augmentation.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"core/#ExaModels","page":"API Manual","title":"ExaModels","text":"","category":"section"},{"location":"core/#ExaModels.ExaModels","page":"API Manual","title":"ExaModels.ExaModels","text":"ExaModels\n\nAn algebraic modeling and automatic differentiation tool in Julia Language, specialized for SIMD abstraction of nonlinear programs.\n\nFor more information, please visit https://github.com/exanauts/ExaModels.jl\n\n\n\n\n\n","category":"module"},{"location":"core/#ExaModels.AdjointNode1","page":"API Manual","title":"ExaModels.AdjointNode1","text":"AdjointNode1{F, T, I}\n\nA node with one child for first-order forward pass tree\n\nFields:\n\nx::T: function value\ny::T: first-order sensitivity\ninner::I: children\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.AdjointNode2","page":"API Manual","title":"ExaModels.AdjointNode2","text":"AdjointNode2{F, T, I1, I2}\n\nA node with two children for first-order forward pass tree\n\nFields:\n\nx::T: function value\ny1::T: first-order sensitivity w.r.t. first argument\ny2::T: first-order sensitivity w.r.t. second argument\ninner1::I1: children #1\ninner2::I2: children #2\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.AdjointNodeSource","page":"API Manual","title":"ExaModels.AdjointNodeSource","text":"AdjointNodeSource{VT}\n\nA source of AdjointNode. adjoint_node_source[i] returns an AdjointNodeVar at index i.\n\nFields:\n\ninner::VT: variable vector\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.AdjointNodeVar","page":"API Manual","title":"ExaModels.AdjointNodeVar","text":"AdjointNodeVar{I, T}\n\nA variable node for first-order forward pass tree\n\nFields:\n\ni::I: index\nx::T: value\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.AdjointNull","page":"API Manual","title":"ExaModels.AdjointNull","text":"Null\n\nA null node\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.Compressor","page":"API Manual","title":"ExaModels.Compressor","text":"Compressor{I}\n\nData structure for the sparse index\n\nFields:\n\ninner::I: stores the sparse index as a tuple form\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.ExaCore","page":"API Manual","title":"ExaModels.ExaCore","text":"ExaCore([array_eltype::Type; backend = backend, minimize = true])\n\nReturns an intermediate data object ExaCore, which later can be used for creating ExaModel\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore()\nAn ExaCore\n\n  Float type: ...................... Float64\n  Array type: ...................... Vector{Float64}\n  Backend: ......................... Nothing\n\n  number of objective patterns: .... 0\n  number of constraint patterns: ... 0\n\njulia> c = ExaCore(Float32)\nAn ExaCore\n\n  Float type: ...................... Float32\n  Array type: ...................... Vector{Float32}\n  Backend: ......................... Nothing\n\n  number of objective patterns: .... 0\n  number of constraint patterns: ... 0\n\njulia> using CUDA\n\njulia> c = ExaCore(Float32; backend = CUDABackend())\nAn ExaCore\n\n  Float type: ...................... Float32\n  Array type: ...................... CUDA.CuArray{Float32, 1, CUDA.DeviceMemory}\n  Backend: ......................... CUDA.CUDAKernels.CUDABackend\n\n  number of objective patterns: .... 0\n  number of constraint patterns: ... 0\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.ExaModel-Tuple{C} where C<:ExaCore","page":"API Manual","title":"ExaModels.ExaModel","text":"ExaModel(core)\n\nReturns an ExaModel object, which can be solved by nonlinear optimization solvers within JuliaSmoothOptimizer ecosystem, such as NLPModelsIpopt or MadNLP.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();                      # create an ExaCore object\n\njulia> x = variable(c, 1:10);              # create variables\n\njulia> objective(c, x[i]^2 for i in 1:10); # set objective function\n\njulia> m = ExaModel(c)                     # create an ExaModel object\nAn ExaModel{Float64, Vector{Float64}, ...}\n\n  Problem name: Generic\n   All variables: ████████████████████ 10     All constraints: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n            free: ████████████████████ 10                free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n            nnzh: ( 81.82% sparsity)   10              linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n                                                    nonlinear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0\n                                                         nnzj: (------% sparsity)\n                                                     lin_nnzj: (------% sparsity)\n                                                     nln_nnzj: (------% sparsity)\n\njulia> using NLPModelsIpopt\n\njulia> result = ipopt(m; print_level=0)    # solve the problem\n\"Execution stats: first-order stationary\"\n\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.Node1","page":"API Manual","title":"ExaModels.Node1","text":"Node1{F, I}\n\nA node with one child for symbolic expression tree\n\nFields:\n\ninner::I: children\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.Node2","page":"API Manual","title":"ExaModels.Node2","text":"Node2{F, I1, I2}\n\nA node with two children for symbolic expression tree\n\nFields:\n\ninner1::I1: children #1\ninner2::I2: children #2\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.Null","page":"API Manual","title":"ExaModels.Null","text":"Null\n\nA null node\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.ParIndexed","page":"API Manual","title":"ExaModels.ParIndexed","text":"ParIndexed{I, J}\n\nA parameterized data node\n\nFields:\n\ninner::I: parameter for the data\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.ParSource","page":"API Manual","title":"ExaModels.ParSource","text":"ParSource\n\nA source of parameterized data\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.ParameterSubexpr","page":"API Manual","title":"ExaModels.ParameterSubexpr","text":"ParameterSubexpr\n\nA parameter-only subexpression whose values are computed once when parameters are set, not at every function evaluation. Use this for expressions that depend only on parameters (θ), not on variables (x). Values are automatically recomputed when set_parameter! is called.\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.ReducedSubexpr","page":"API Manual","title":"ExaModels.ReducedSubexpr","text":"ReducedSubexpr\n\nA reduced-form subexpression that substitutes the expression directly when indexed. No auxiliary variables or constraints are created - the expression is inlined.\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.SIMDFunction","page":"API Manual","title":"ExaModels.SIMDFunction","text":"SIMDFunction(gen::Base.Generator, o0 = 0, o1 = 0, o2 = 0)\n\nReturns a SIMDFunction using the gen.\n\nArguments:\n\ngen: an iterable function specified in Base.Generator format\no0: offset for the function evaluation\no1: offset for the derivative evalution\no2: offset for the second-order derivative evalution\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.SecondAdjointNode1","page":"API Manual","title":"ExaModels.SecondAdjointNode1","text":"SecondAdjointNode1{F, T, I}\n\nA node with one child for second-order forward pass tree\n\nFields:\n\nx::T: function value\ny::T: first-order sensitivity\nh::T: second-order sensitivity\ninner::I: DESCRIPTION\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.SecondAdjointNode2","page":"API Manual","title":"ExaModels.SecondAdjointNode2","text":"SecondAdjointNode2{F, T, I1, I2}\n\nA node with one child for second-order forward pass tree\n\nFields:\n\nx::T: function value\ny1::T: first-order sensitivity w.r.t. first argument\ny2::T: first-order sensitivity w.r.t. first argument\nh11::T: second-order sensitivity w.r.t. first argument\nh12::T: second-order sensitivity w.r.t. first and second argument\nh22::T: second-order sensitivity w.r.t. second argument\ninner1::I1: children #1\ninner2::I2: children #2\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.SecondAdjointNodeSource","page":"API Manual","title":"ExaModels.SecondAdjointNodeSource","text":"SecondAdjointNodeSource{VT}\n\nA source of AdjointNode. adjoint_node_source[i] returns an AdjointNodeVar at index i.\n\nFields:\n\ninner::VT: variable vector\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.SecondAdjointNodeVar","page":"API Manual","title":"ExaModels.SecondAdjointNodeVar","text":"SecondAdjointNodeVar{I, T}\n\nA variable node for first-order forward pass tree\n\nFields:\n\ni::I: index\nx::T: value\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.SecondAdjointNull","page":"API Manual","title":"ExaModels.SecondAdjointNull","text":"Null\n\nA null node\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.Subexpr","page":"API Manual","title":"ExaModels.Subexpr","text":"Subexpr\n\nA subexpression that has been lifted to auxiliary variables with defining equality constraints. Can be indexed like a Variable to get Var nodes for use in objectives and constraints.\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.TwoStageExaModel","page":"API Manual","title":"ExaModels.TwoStageExaModel","text":"TwoStageExaModel{T, VT, M}\n\nA two-stage optimization model where all scenarios are fused into a single ExaModel. Evaluates all scenarios in ONE kernel launch.\n\nFields\n\nmodel::M: Single fused ExaModel containing all scenarios\nns::Int: Number of scenarios\nnv::Int: Recourse variables per scenario\nnd::Int: Design (shared) variables\nnc::Int: Constraints per scenario\nnθ::Int: Parameters per scenario\nnnzj_per_scenario::Int: Jacobian nonzeros per scenario (approximate)\nnnzh_per_scenario::Int: Hessian nonzeros per scenario (approximate)\n\nStructure\n\nTotal variables: ns*nv + nd\nTotal constraints: ns*nc\nGlobal variable layout: [v₁; v₂; ...; vₛ; d]\nGlobal constraint layout: [c₁; c₂; ...; cₛ]\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.TwoStageExaModel-Tuple{Function, Int64, Int64, Int64, Vector{<:AbstractVector}}","page":"API Manual","title":"ExaModels.TwoStageExaModel","text":"TwoStageExaModel(build, nd, nv, ns, θ_sets; backend=nothing)\n\nBuild a two-stage model where all scenarios are fused into a single ExaModel.\n\nAll scenarios share ONE compiled expression pattern, achieving maximum GPU efficiency. This requires scenarios to have identical structure.\n\nArguments\n\nbuild::Function: Function (c, d, v, θ, ns, nv, nθ) -> nothing\nc: ExaCore\nd: Variable handle for design variables (indices 1:nd)\nv: Variable handle for ALL recourse variables (indices 1:nsnv)     Scenario i's vars are at indices (i-1)nv+1 : i*nv\nθ: Parameter handle for ALL parameters (length nsnθ)     Scenario i's params are at indices (i-1)nθ+1 : i*nθ\nns, nv, nθ: dimensions for building iteration data\nnd::Int: Number of design variables\nnv::Int: Number of recourse variables per scenario\nns::Int: Number of scenarios\nθ_sets::Vector{<:AbstractVector}: Parameter vectors for each scenario\n\nKeyword Arguments\n\nbackend: Backend for computation (default: nothing)\nd_start: Initial values for design variables (scalar or vector of length nd, default: 0.0)\nd_lvar: Lower bounds for design variables (scalar or vector of length nd, default: -Inf)\nd_uvar: Upper bounds for design variables (scalar or vector of length nd, default: Inf)\nv_start: Initial values for recourse variables (scalar or vector of length ns*nv, default: 0.0)\nv_lvar: Lower bounds for recourse variables (scalar or vector of length ns*nv, default: -Inf)\nv_uvar: Upper bounds for recourse variables (scalar or vector of length ns*nv, default: Inf)\n\nExample\n\nns, nv, nd, nθ = 100, 5, 2, 3\nθ_sets = [rand(nθ) for _ in 1:ns]\n\nmodel = TwoStageExaModel(nd, nv, ns, θ_sets) do c, d, v, θ, ns, nv, nθ\n    obj_data = [(i, j, (i-1)*nv + j, (i-1)*nθ) for i in 1:ns for j in 1:nv]\n    objective(c, θ[θ_off + 1] * v[v_idx]^2 for (i, j, v_idx, θ_off) in obj_data)\n\n    con_data = [(i, j, (i-1)*nv + j, (i-1)*nθ) for i in 1:ns for j in 1:nv]\n    constraint(c, v[v_idx] + d[1] - θ[θ_off + 3] for (i, j, v_idx, θ_off) in con_data)\nend\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.Var","page":"API Manual","title":"ExaModels.Var","text":"Var{I}\n\nA variable node for symbolic expression tree\n\nFields:\n\ni::I: (parameterized) index \n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.VarSource","page":"API Manual","title":"ExaModels.VarSource","text":"VarSource\n\nA source of variable nodes\n\n\n\n\n\n","category":"type"},{"location":"core/#ExaModels.WrapperNLPModel-Tuple{Any, Any}","page":"API Manual","title":"ExaModels.WrapperNLPModel","text":"WrapperNLPModel(VT, m)\n\nReturns a WrapperModel{T,VT} wrapping m <: AbstractNLPModel{T}\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.WrapperNLPModel-Tuple{Any}","page":"API Manual","title":"ExaModels.WrapperNLPModel","text":"WrapperNLPModel(m)\n\nReturns a WrapperModel{Float64,Vector{64}} wrapping m\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels._recompute_param_subexprs!-Tuple{ExaCore}","page":"API Manual","title":"ExaModels._recompute_param_subexprs!","text":"_recompute_param_subexprs!(c::ExaCore)\n\nRe-evaluates all parameter-only subexpressions and updates their cached values in θ. Called automatically by set_parameter!.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.cons_block_indices-Tuple{TwoStageExaModel, Int64}","page":"API Manual","title":"ExaModels.cons_block_indices","text":"cons_block_indices(model::TwoStageExaModel, i) -> UnitRange\n\nGet the index range for constraints of scenario i in the global constraint vector. Use as: c_global[cons_block_indices(model, i)]\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.constraint!-Union{Tuple{C}, Tuple{C, Any, Any, Any}} where C<:ExaCore","page":"API Manual","title":"ExaModels.constraint!","text":"constraint!(c, c1, expr, pars)\n\nExpands the existing constraint c1 in c by adding addtional constraints terms specified by expr and pars.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.constraint!-Union{Tuple{C}, Tuple{C, Any, Base.Generator}} where C<:ExaCore","page":"API Manual","title":"ExaModels.constraint!","text":"constraint!(c::C, c1, gen::Base.Generator) where {C<:ExaCore}\n\nExpands the existing constraint c1 in c by adding additional constraint terms specified by a generator.\n\nArguments\n\nc::C: The model to which the constraints are added.\nc1: An initial constraint value or expression.\ngen::Base.Generator: A generator that produces the pair of constraint index and term to be added.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 10);\n\njulia> c1 = constraint(c, x[i] + x[i+1] for i=1:9; lcon = -1, ucon = (1+i for i=1:9));\n\njulia> constraint!(c, c1, i => sin(x[i+1]) for i=4:6)\nConstraint Augmentation\n\n  s.t. (...)\n       g♭ ≤ (...) + ∑_{p ∈ P} h(x,θ,p) ≤ g♯\n\n  where |P| = 3\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.constraint-Union{Tuple{C}, Tuple{T}, Tuple{C, Any}} where {T, C<:(ExaCore{T, VT} where VT<:AbstractVector{T})}","page":"API Manual","title":"ExaModels.constraint","text":"constraint(core, n; start = 0, lcon = 0,  ucon = 0)\n\nAdds empty constraints of dimension n, so that later the terms can be added with constraint!.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.constraint-Union{Tuple{C}, Tuple{T}, Tuple{C, Base.Generator}} where {T, C<:(ExaCore{T, VT} where VT<:AbstractVector{T})}","page":"API Manual","title":"ExaModels.constraint","text":"constraint(core, generator; start = 0, lcon = 0,  ucon = 0)\n\nAdds constraints specified by a generator to core, and returns an Constraint object.\n\nKeyword Arguments\n\nstart: The initial guess of the dual solution. Can either be Number, AbstractArray, or Generator.\nlcon : The constraint lower bound. Can either be Number, AbstractArray, or Generator.\nucon : The constraint upper bound. Can either be Number, AbstractArray, or Generator.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 10);\n\njulia> constraint(c, x[i] + x[i+1] for i=1:9; lcon = -1, ucon = (1+i for i=1:9))\nConstraint\n\n  s.t. (...)\n       g♭ ≤ [g(x,θ,p)]_{p ∈ P} ≤ g♯\n\n  where |P| = 9\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.constraint-Union{Tuple{N}, Tuple{C}, Tuple{T}, Tuple{C, N}, Tuple{C, N, Any}} where {T, C<:(ExaCore{T, VT} where VT<:AbstractVector{T}), N<:ExaModels.AbstractNode}","page":"API Manual","title":"ExaModels.constraint","text":"constraint(core, expr [, pars]; start = 0, lcon = 0,  ucon = 0)\n\nAdds constraints specified by a expr and pars to core, and returns an Constraint object.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.design_var_index-Tuple{TwoStageExaModel, Int64}","page":"API Manual","title":"ExaModels.design_var_index","text":"design_var_index(model, j) -> global_idx\n\nGet global index for design variable j.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.design_var_indices-Tuple{TwoStageExaModel}","page":"API Manual","title":"ExaModels.design_var_indices","text":"design_var_indices(model::TwoStageExaModel) -> UnitRange\n\nGet the index range for design variables in the global variable vector. Use as: x_global[design_var_indices(model)]\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.drpass-Union{Tuple{D}, Tuple{D, Any, Any}} where D<:ExaModels.AdjointNull","page":"API Manual","title":"ExaModels.drpass","text":"drpass(d::D, y, adj)\n\nPerforms dense gradient evaluation via the reverse pass on the computation (sub)graph formed by forward pass\n\nArguments:\n\nd: first-order computation (sub)graph\ny: result vector\nadj: adjoint propagated up to the current node\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.extract_cons_block!-Tuple{AbstractVector, TwoStageExaModel, Int64, AbstractVector}","page":"API Manual","title":"ExaModels.extract_cons_block!","text":"extract_cons_block!(dest, model::TwoStageExaModel, i, c_global)\n\nExtract constraint block for scenario i into pre-allocated dest. Returns dest for convenience.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.extract_design_vars!-Tuple{AbstractVector, TwoStageExaModel, AbstractVector}","page":"API Manual","title":"ExaModels.extract_design_vars!","text":"extract_design_vars!(dest, model::TwoStageExaModel, x_global)\n\nExtract design variables into pre-allocated dest. Returns dest for convenience.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.extract_grad_block!-Tuple{AbstractVector, AbstractVector, TwoStageExaModel, Int64, AbstractVector}","page":"API Manual","title":"ExaModels.extract_grad_block!","text":"extract_grad_block!(g_v, g_d, model::TwoStageExaModel, i, g_global)\n\nExtract gradient block for scenario i into pre-allocated g_v and g_d. Returns (g_v, g_d) for convenience.\n\nNote: The design variable gradient accumulates contributions from all scenarios.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.extract_recourse_vars!-Tuple{AbstractVector, TwoStageExaModel, Int64, AbstractVector}","page":"API Manual","title":"ExaModels.extract_recourse_vars!","text":"extract_recourse_vars!(dest, model::TwoStageExaModel, i, x_global)\n\nExtract recourse variables for scenario i into pre-allocated dest. Returns dest for convenience.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.get_model-Tuple{TwoStageExaModel}","page":"API Manual","title":"ExaModels.get_model","text":"get_model(model::TwoStageExaModel)\n\nGet the underlying ExaModel for direct NLPModels interface usage.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.global_con_index-Tuple{TwoStageExaModel, Int64, Int64}","page":"API Manual","title":"ExaModels.global_con_index","text":"global_con_index(model, i, local_idx) -> global_idx\n\nConvert local constraint index to global index for scenario i.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.global_var_index-Tuple{TwoStageExaModel, Int64, Int64}","page":"API Manual","title":"ExaModels.global_var_index","text":"global_var_index(model, i, local_idx) -> global_idx\n\nConvert local variable index to global index for scenario i.\n\nLocal ordering (for scenario API): [d₁, ..., dnd, v₁, ..., vnv] Global ordering: [v₁¹...vnv¹, v₁²...vnv², ..., v₁ⁿˢ...vnvⁿˢ, d₁...dnd]\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.grad_design_indices-Tuple{TwoStageExaModel}","page":"API Manual","title":"ExaModels.grad_design_indices","text":"grad_design_indices(model::TwoStageExaModel) -> UnitRange\n\nGet the index range for design gradient. Same as design_var_indices since gradient has same layout as variables.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.grad_recourse_indices-Tuple{TwoStageExaModel, Int64}","page":"API Manual","title":"ExaModels.grad_recourse_indices","text":"grad_recourse_indices(model::TwoStageExaModel, i) -> UnitRange\n\nGet the index range for recourse gradient of scenario i. Same as recourse_var_indices since gradient has same layout as variables.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.gradient!-NTuple{5, Any}","page":"API Manual","title":"ExaModels.gradient!","text":"gradient!(y, f, x, adj)\n\nPerforms dense gradient evalution\n\nArguments:\n\ny: result vector\nf: the function to be differentiated in SIMDFunction format\nx: variable vector\nadj: initial adjoint\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.grpass-Union{Tuple{D}, Tuple{D, Vararg{Any, 5}}} where D<:Union{ExaModels.AdjointNull, Real, ExaModels.ParIndexed}","page":"API Manual","title":"ExaModels.grpass","text":"grpass(d::D, comp, y, o1, cnt, adj)\n\nPerforms dsparse gradient evaluation via the reverse pass on the computation (sub)graph formed by forward pass\n\nArguments:\n\nd: first-order computation (sub)graph\ncomp: a Compressor, which helps map counter to sparse vector index\ny: result vector\no1: index offset\ncnt: counter\nadj: adjoint propagated up to the current node\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.hdrpass-Union{Tuple{T2}, Tuple{T1}, Tuple{T1, T2, Vararg{Any, 6}}} where {T1<:ExaModels.SecondAdjointNode1, T2<:ExaModels.SecondAdjointNode1}","page":"API Manual","title":"ExaModels.hdrpass","text":"hdrpass(t1::T1, t2::T2, comp, y1, y2, o2, cnt, adj)\n\nPerforms sparse hessian evaluation ((df1/dx)(df2/dx)' portion) via the reverse pass on the computation (sub)graph formed by second-order forward pass\n\nArguments:\n\nt1: second-order computation (sub)graph regarding f1\nt2: second-order computation (sub)graph regarding f2\ncomp: a Compressor, which helps map counter to sparse vector index\ny1: result vector #1\ny2: result vector #2 (only used when evaluating sparsity)\no2: index offset\ncnt: counter\nadj: second adjoint propagated up to the current node\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.jrpass-Union{Tuple{D}, Tuple{D, Vararg{Any, 7}}} where D<:Union{ExaModels.AdjointNull, Real}","page":"API Manual","title":"ExaModels.jrpass","text":"jrpass(d::D, comp, i, y1, y2, o1, cnt, adj)\n\nPerforms sparse jacobian evaluation via the reverse pass on the computation (sub)graph formed by forward pass\n\nArguments:\n\nd: first-order computation (sub)graph\ncomp: a Compressor, which helps map counter to sparse vector index\ni: constraint index (this is i-th constraint)\ny1: result vector #1\ny2: result vector #2 (only used when evaluating sparsity)\no1: index offset\ncnt: counter\nadj: adjoint propagated up to the current node\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.multipliers-Tuple{SolverCore.AbstractExecutionStats, ExaModels.Constraint}","page":"API Manual","title":"ExaModels.multipliers","text":"multipliers(result, y)\n\nReturns the multipliers for constraints y associated with result, obtained by solving the model.\n\nExample\n\njulia> using ExaModels, NLPModelsIpopt\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 1:10, lvar = -1, uvar = 1);\n\njulia> objective(c, (x[i]-2)^2 for i in 1:10);\n\njulia> y = constraint(c, x[i] + x[i+1] for i=1:9; lcon = -1, ucon = (1+i for i=1:9));\n\njulia> m = ExaModel(c);\n\njulia> result = ipopt(m; print_level=0);\n\njulia> val = multipliers(result, y);\n\n\njulia> val[1] ≈ 0.81933930\ntrue\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.multipliers_L-Tuple{SolverCore.AbstractExecutionStats, Any}","page":"API Manual","title":"ExaModels.multipliers_L","text":"multipliers_L(result, x)\n\nReturns the multipliers_L for variable x associated with result, obtained by solving the model.\n\nExample\n\njulia> using ExaModels, NLPModelsIpopt\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 1:10, lvar = -1, uvar = 1);\n\njulia> objective(c, (x[i]-2)^2 for i in 1:10);\n\njulia> m = ExaModel(c);\n\njulia> result = ipopt(m; print_level=0);\n\njulia> val = multipliers_L(result, x);\n\njulia> isapprox(val, fill(0, 10), atol=sqrt(eps(Float64)), rtol=Inf)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.multipliers_U-Tuple{SolverCore.AbstractExecutionStats, Any}","page":"API Manual","title":"ExaModels.multipliers_U","text":"multipliers_U(result, x)\n\nReturns the multipliers_U for variable x associated with result, obtained by solving the model.\n\nExample\n\njulia> using ExaModels, NLPModelsIpopt\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 1:10, lvar = -1, uvar = 1);\n\njulia> objective(c, (x[i]-2)^2 for i in 1:10);\n\njulia> m = ExaModel(c);\n\njulia> result = ipopt(m; print_level=0);\n\njulia> val = multipliers_U(result, x);\n\njulia> isapprox(val, fill(2, 10), atol=sqrt(eps(Float64)), rtol=Inf)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.objective-Union{Tuple{C}, Tuple{C, Any}} where C<:ExaCore","page":"API Manual","title":"ExaModels.objective","text":"objective(core::ExaCore, generator)\n\nAdds objective terms specified by a generator to core, and returns an Objective object. Note: it is assumed that the terms are summed.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 10);\n\njulia> objective(c, x[i]^2 for i=1:10)\nObjective\n\n  min (...) + ∑_{p ∈ P} f(x,θ,p)\n\n  where |P| = 10\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.objective-Union{Tuple{N}, Tuple{C}, Tuple{C, N}, Tuple{C, N, Any}} where {C<:ExaCore, N<:ExaModels.AbstractNode}","page":"API Manual","title":"ExaModels.objective","text":"objective(core::ExaCore, expr [, pars])\n\nAdds objective terms specified by a expr and pars to core, and returns an Objective object.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.parameter-Union{Tuple{C}, Tuple{T}, Tuple{C, AbstractArray}} where {T, C<:(ExaCore{T, VT} where VT<:AbstractVector{T})}","page":"API Manual","title":"ExaModels.parameter","text":"parameter(core, start::AbstractArray)\n\nAdds parameters with initial values specified by start, and returns Parameter object.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();\n\njulia> θ = parameter(c, ones(10))\nParameter\n\n  θ ∈ R^{10}\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.recourse_var_index-Tuple{TwoStageExaModel, Int64, Int64}","page":"API Manual","title":"ExaModels.recourse_var_index","text":"recourse_var_index(model, i, j) -> global_idx\n\nGet global index for recourse variable j of scenario i.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.recourse_var_indices-Tuple{TwoStageExaModel, Int64}","page":"API Manual","title":"ExaModels.recourse_var_indices","text":"recourse_var_indices(model::TwoStageExaModel, i) -> UnitRange\n\nGet the index range for recourse variables of scenario i in the global variable vector. Use as: x_global[recourse_var_indices(model, i)]\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.set_all_scenario_parameters!-Tuple{TwoStageExaModel, Vector{<:AbstractVector}}","page":"API Manual","title":"ExaModels.set_all_scenario_parameters!","text":"set_all_scenario_parameters!(model, θ_sets)\n\nUpdate parameters for all scenarios.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.set_parameter!-Tuple{ExaCore, ExaModels.Parameter, AbstractArray}","page":"API Manual","title":"ExaModels.set_parameter!","text":"set_parameter!(core, param, values)\n\nUpdates the values of parameters in the core.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();\n\njulia> p = parameter(c, ones(5))\nParameter\n\n  θ ∈ R^{5}\n\njulia> set_parameter!(c, p, rand(5))  # Update with new values\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.set_scenario_parameters!-Tuple{TwoStageExaModel, Int64, AbstractVector}","page":"API Manual","title":"ExaModels.set_scenario_parameters!","text":"set_scenario_parameters!(model, i, θ_new)\n\nUpdate parameters for scenario i.\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.sgradient!-NTuple{5, Any}","page":"API Manual","title":"ExaModels.sgradient!","text":"sgradient!(y, f, x, adj)\n\nPerforms sparse gradient evalution\n\nArguments:\n\ny: result vector\nf: the function to be differentiated in SIMDFunction format\nx: variable vector\nadj: initial adjoint\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.shessian!-NTuple{7, Any}","page":"API Manual","title":"ExaModels.shessian!","text":"shessian!(y1, y2, f, x, adj1, adj2)\n\nPerforms sparse jacobian evalution\n\nArguments:\n\ny1: result vector #1\ny2: result vector #2 (only used when evaluating sparsity)\nf: the function to be differentiated in SIMDFunction format\nx: variable vector\nadj1: initial first adjoint\nadj2: initial second adjoint\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.sjacobian!-NTuple{6, Any}","page":"API Manual","title":"ExaModels.sjacobian!","text":"sjacobian!(y1, y2, f, x, adj)\n\nPerforms sparse jacobian evalution\n\nArguments:\n\ny1: result vector #1\ny2: result vector #2 (only used when evaluating sparsity)\nf: the function to be differentiated in SIMDFunction format\nx: variable vector\nadj: initial adjoint\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.solution-Tuple{SolverCore.AbstractExecutionStats, Any}","page":"API Manual","title":"ExaModels.solution","text":"solution(result, x)\n\nReturns the solution for variable x associated with result, obtained by solving the model.\n\nExample\n\njulia> using ExaModels, NLPModelsIpopt\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 1:10, lvar = -1, uvar = 1);\n\njulia> objective(c, (x[i]-2)^2 for i in 1:10);\n\njulia> m = ExaModel(c);\n\njulia> result = ipopt(m; print_level=0);\n\njulia> val = solution(result, x);\n\njulia> isapprox(val, fill(1, 10), atol=sqrt(eps(Float64)), rtol=Inf)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.subexpr-Union{Tuple{C}, Tuple{T}, Tuple{C, Base.Generator}} where {T, C<:(ExaCore{T, VT} where VT<:AbstractVector{T})}","page":"API Manual","title":"ExaModels.subexpr","text":"subexpr(core, generator; reduced=false, parameter_only=false)\n\nCreates a subexpression that can be reused in objectives and constraints.\n\nThree forms are available:\n\nLifted (default, reduced=false): Creates auxiliary variables with defining equality constraints. This generates derivative code once and uses simple variable references thereafter. Adds variables and constraints to the problem.\nReduced (reduced=true): Stores the expression for direct substitution when indexed. No auxiliary variables or constraints are created. The expression is inlined wherever used.\nParameter-only (parameter_only=true): For expressions that depend only on parameters (θ), not variables (x). Values are computed once when parameters are set, not at every function evaluation. Automatically recomputed when set_parameter! is called.\n\nBoth lifted and reduced forms support SIMD-vectorized evaluation and can be nested.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 10);\n\njulia> s = subexpr(c, x[i]^2 for i in 1:10)\nSubexpression (lifted)\n\n  s ∈ R^{10}\n\njulia> objective(c, s[i] + s[i+1] for i in 1:9);\n\nReduced form (experimental)\n\nwarning: Warning\nThe reduced form (reduced=true) is experimental and may have issues with complex nested expressions. Use the default lifted form for production code.\n\nc = ExaCore()\nx = variable(c, 10)\n\n# Reduced form - no extra variables/constraints\ns = subexpr(c, x[i]^2 for i in 1:10; reduced=true)\n\n# s[i] substitutes x[i]^2 directly into the expression\nobjective(c, s[i] + s[i+1] for i in 1:9)\n\nParameter-only form\n\nFor expressions involving only parameters, use parameter_only=true to evaluate them once when parameters change, rather than at every optimization iteration:\n\nc = ExaCore()\nθ = parameter(c, ones(10))\nx = variable(c, 10)\n\n# Parameter-only subexpression - computed once per parameter update\nweights = subexpr(c, θ[i]^2 + θ[i+1] for i in 1:9; parameter_only=true)\n\n# Use in objective - weights[i] returns cached value, not re-computed\nobjective(c, weights[i] * x[i]^2 for i in 1:9)\n\nMulti-dimensional example\n\nc = ExaCore()\nx = variable(c, 0:T, 0:N)\n\n# Automatically infers 2D structure from Cartesian product\ndx = subexpr(c, x[t, i] - x[t-1, i] for t in 1:T, i in 1:N)\n\n# Now dx[t, i] can be used in constraints\nconstraint(c, dx[t, i] - something for t in 1:T, i in 1:N)\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.variable-Union{Tuple{C}, Tuple{T}, Tuple{C, Vararg{Any}}} where {T, C<:(ExaCore{T, VT} where VT<:AbstractVector{T})}","page":"API Manual","title":"ExaModels.variable","text":"variable(core, dims...; start = 0, lvar = -Inf, uvar = Inf)\n\nAdds variables with dimensions specified by dims to core, and returns Variable object. dims can be either Integer or UnitRange.\n\nKeyword Arguments\n\nstart: The initial guess of the solution. Can either be Number, AbstractArray, or Generator.\nlvar : The variable lower bound. Can either be Number, AbstractArray, or Generator.\nuvar : The variable upper bound. Can either be Number, AbstractArray, or Generator.\n\nExample\n\njulia> using ExaModels\n\njulia> c = ExaCore();\n\njulia> x = variable(c, 10; start = (sin(i) for i=1:10))\nVariable\n\n  x ∈ R^{10}\n\njulia> y = variable(c, 2:10, 3:5; lvar = zeros(9,3), uvar = ones(9,3))\nVariable\n\n  x ∈ R^{9 × 3}\n\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.cons!-Tuple{TwoStageExaModel, AbstractVector, AbstractVector}","page":"API Manual","title":"NLPModels.cons!","text":"cons!(model::TwoStageExaModel, x_global, c_global)\n\nEvaluate all constraints. Output: c_global ∈ ℝ^{ns*nc}\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.get_ncon-Tuple{TwoStageExaModel}","page":"API Manual","title":"NLPModels.get_ncon","text":"get_ncon(model::TwoStageExaModel)\n\nTotal number of constraints.\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.get_nnzh-Tuple{TwoStageExaModel}","page":"API Manual","title":"NLPModels.get_nnzh","text":"get_nnzh(model::TwoStageExaModel)\n\nTotal number of Hessian nonzeros.\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.get_nnzj-Tuple{TwoStageExaModel}","page":"API Manual","title":"NLPModels.get_nnzj","text":"get_nnzj(model::TwoStageExaModel)\n\nTotal number of Jacobian nonzeros.\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.get_nvar-Tuple{TwoStageExaModel}","page":"API Manual","title":"NLPModels.get_nvar","text":"get_nvar(model::TwoStageExaModel)\n\nTotal number of variables.\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.grad!-Tuple{TwoStageExaModel, AbstractVector, AbstractVector}","page":"API Manual","title":"NLPModels.grad!","text":"grad!(model::TwoStageExaModel, x_global, g_global)\n\nEvaluate total gradient. Output: g_global ∈ ℝ^{ns*nv + nd}\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.hess_coord!-Tuple{TwoStageExaModel, AbstractVector, AbstractVector, AbstractVector}","page":"API Manual","title":"NLPModels.hess_coord!","text":"hess_coord!(model::TwoStageExaModel, x_global, y_global, hess_global; obj_weight=1.0)\n\nEvaluate full Hessian of Lagrangian (COO values).\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.hess_structure!-Tuple{TwoStageExaModel, AbstractVector{<:Integer}, AbstractVector{<:Integer}}","page":"API Manual","title":"NLPModels.hess_structure!","text":"hess_structure!(model::TwoStageExaModel, rows, cols)\n\nGet full Hessian sparsity structure.\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.jac_coord!-Tuple{TwoStageExaModel, AbstractVector, AbstractVector}","page":"API Manual","title":"NLPModels.jac_coord!","text":"jac_coord!(model::TwoStageExaModel, x_global, jac_global)\n\nEvaluate full Jacobian (COO values).\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.jac_structure!-Tuple{TwoStageExaModel, AbstractVector{<:Integer}, AbstractVector{<:Integer}}","page":"API Manual","title":"NLPModels.jac_structure!","text":"jac_structure!(model::TwoStageExaModel, rows, cols)\n\nGet full Jacobian sparsity structure.\n\n\n\n\n\n","category":"method"},{"location":"core/#NLPModels.obj-Tuple{TwoStageExaModel, AbstractVector}","page":"API Manual","title":"NLPModels.obj","text":"obj(model::TwoStageExaModel, x_global)\n\nEvaluate total objective (sum over all scenarios).\n\n\n\n\n\n","category":"method"},{"location":"core/#ExaModels.@register_bivariate-NTuple{6, Any}","page":"API Manual","title":"ExaModels.@register_bivariate","text":"register_bivariate(f, df1, df2, ddf11, ddf12, ddf22)\n\nRegister a bivariate function f to ExaModels, so that it can be used within objective and constraint expressions\n\nArguments:\n\nf: function\ndf1: derivative function (w.r.t. first argument)\ndf2: derivative function (w.r.t. second argument)\nddf11: second-order derivative funciton (w.r.t. first argument)\nddf12: second-order derivative funciton (w.r.t. first and second argument)\nddf22: second-order derivative funciton (w.r.t. second argument)\n\nExample\n\njulia> using ExaModels\n\njulia> relu23(x,y) = (x > 0 || y > 0) ? (x + y)^3 : zero(x)\nrelu23 (generic function with 1 method)\n\njulia> drelu231(x,y) = (x > 0 || y > 0) ? 3 * (x + y)^2 : zero(x)\ndrelu231 (generic function with 1 method)\n\njulia> drelu232(x,y) = (x > 0 || y > 0) ? 3 * (x + y)^2  : zero(x)\ndrelu232 (generic function with 1 method)\n\njulia> ddrelu2311(x,y) = (x > 0 || y > 0) ? 6 * (x + y) : zero(x)\nddrelu2311 (generic function with 1 method)\n\njulia> ddrelu2312(x,y) = (x > 0 || y > 0) ? 6 * (x + y) : zero(x)\nddrelu2312 (generic function with 1 method)\n\njulia> ddrelu2322(x,y) = (x > 0 || y > 0) ? 6 * (x + y) : zero(x)\nddrelu2322 (generic function with 1 method)\n\njulia> @register_bivariate(relu23, drelu231, drelu232, ddrelu2311, ddrelu2312, ddrelu2322)\n\n\n\n\n\n","category":"macro"},{"location":"core/#ExaModels.@register_univariate-Tuple{Any, Any, Any}","page":"API Manual","title":"ExaModels.@register_univariate","text":"@register_univariate(f, df, ddf)\n\nRegister a univariate function f to ExaModels, so that it can be used within objective and constraint expressions\n\nArguments:\n\nf: function\ndf: derivative function\nddf: second-order derivative funciton\n\nExample\n\njulia> using ExaModels\n\njulia> relu3(x) = x > 0 ? x^3 : zero(x)\nrelu3 (generic function with 1 method)\n\njulia> drelu3(x) = x > 0 ? 3*x^2 : zero(x)\ndrelu3 (generic function with 1 method)\n\njulia> ddrelu3(x) = x > 0 ? 6*x : zero(x)\nddrelu3 (generic function with 1 method)\n\njulia> @register_univariate(relu3, drelu3, ddrelu3)\n\n\n\n\n\n","category":"macro"},{"location":"simd/#simd","page":"Mathematical Abstraction","title":"SIMD Abstraction","text":"In this page, we explain what SIMD abstraction of nonlinear program is, and why it can be beneficial for scalable optimization of large-scale optimization problems. More discussion can be found in our paper.","category":"section"},{"location":"simd/#What-is-SIMD-abstraction?","page":"Mathematical Abstraction","title":"What is SIMD abstraction?","text":"The mathematical statement of the problem formulation is as follows.\n\nbeginaligned\n  min_x^flatleq x leq x^sharp\n   sum_linLsum_iin I_l f^(l)(x p^(l)_i)\n  textst g^flat leq leftg^(m)(x q_j)right_jin J_m +sum_nin N_msum_kin K_nh^(n)(x s^(n)_k) leq g^sharpquad forall minM\nendaligned\n\nwhere f^(ell)(cdotcdot), g^(m)(cdotcdot), and h^(n)(cdotcdot) are twice differentiable functions with respect to the first argument, whereas p^(k)_i_iin N_k_kinK, q^(k)_i_iin M_l_minM, and s^(n)_k_kinK_n_ninN_m_minM are problem data, which can either be discrete or continuous. It is also assumed that our functions f^(l)(cdotcdot), g^(m)(cdotcdot), and h^(n)(cdotcdot) can be expressed with computational graphs of moderate length. ","category":"section"},{"location":"simd/#Why-SIMD-abstraction?","page":"Mathematical Abstraction","title":"Why SIMD abstraction?","text":"Many physics-based models, such as AC OPF, have a highly repetitive structure. One of the manifestations of it is that the mathematical statement of the model is concise, even if the practical model may contain millions of variables and constraints. This is possible due to the use of repetition over a certain index and data sets. For example, it suffices to use 15 computational patterns to fully specify the AC OPF model. These patterns arise from (1) generation cost, (2) reference bus voltage angle constraint, (3-6) active and reactive power flow (from and to), (7) voltage angle difference constraint, (8-9) apparent power flow limits (from and to), (10-11) power balance equations, (12-13) generators' contributions to the power balance equations, and (14-15) in/out flows contributions to the power balance equations. However, such repetitive structure is not well exploited in the standard NLP modeling paradigms. In fact, without the SIMD abstraction, it is difficult for the AD package to detect the parallelizable structure within the model, as it will require the full inspection of the computational graph over all expressions.  By preserving the repetitive structures in the model, the repetitive structure can be directly available in AD implementation.\n\nUsing the multiple dispatch feature of Julia, ExaModels.jl generates highly efficient derivative computation code, specifically compiled for each computational pattern in the model. These derivative evaluation codes can be run over the data in various GPU array formats, and implemented via array and kernel programming in Julia Language. In turn, ExaModels.jl has the capability to efficiently evaluate first and second-order derivatives using GPU accelerators.","category":"section"},{"location":"jump/#JuMP-Interface-(Experimental)","page":"JuMP Interface (experimental)","title":"JuMP Interface (Experimental)","text":"","category":"section"},{"location":"jump/#JuMP-to-an-ExaModel","page":"JuMP Interface (experimental)","title":"JuMP to an ExaModel","text":"We have an experimental interface to JuMP model. A JuMP model can be directly converted to a ExaModel. It is as simple as this:\n\nusing ExaModels, JuMP, CUDA\n\nN = 10\njm = Model()\n\n@variable(jm, x[i=1:N], start = mod(i, 2) == 1 ? -1.2 : 1.0)\n@constraint(\n    jm,\n    s[i=1:(N-2)],\n    3x[i+1]^3 + 2x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n    x[i]exp(x[i] - x[i+1]) - 3 == 0.0\n)\n@objective(jm, Min, sum(100(x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2 for i = 2:N))\n\nem = ExaModel(jm; backend = CUDABackend())\n\nAn ExaModel{Float64, CUDA.CuArray{Float64, 1, CUDA.DeviceMemory}, ...}\n\n  Problem name: Generic\n   All variables: ████████████████████ 10     All constraints: ████████████████████ 8     \n            free: ████████████████████ 10                free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ████████████████████ 8     \n          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n            nnzh: (-212.73% sparsity)   172             linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n                                                    nonlinear: ████████████████████ 8     \n                                                         nnzj: (  0.00% sparsity)   80    \n                                                     lin_nnzj: (------% sparsity)         \n                                                     nln_nnzj: (  0.00% sparsity)   80    \n\n\n\nHere, note that only scalar objective/constraints created via @constraint and @objective API are supported. Older syntax like @NLconstraint and @NLobjective are not supported. We can solve the model using any of the solvers supported by ExaModels. For example, we can use MadNLP:\n\nusing MadNLPGPU\n\nresult = madnlp(em)\n\n\"Execution stats: Optimal Solution Found (tol = 1.0e-04).\"","category":"section"},{"location":"jump/#JuMP-Optimizer","page":"JuMP Interface (experimental)","title":"JuMP Optimizer","text":"Alternatively, one can use the Optimizer interface provided by ExaModels. This feature can be used as follows.\n\nusing ExaModels, JuMP, CUDA\nusing MadNLPGPU\n\nset_optimizer(jm, () -> ExaModels.MadNLPOptimizer(CUDABackend()))\noptimize!(jm)\n\nThis is MadNLP version v0.8.12, running with cuDSS v0.7.1\n\nNumber of nonzeros in constraint Jacobian............:       80\nNumber of nonzeros in Lagrangian Hessian.............:      172\n\nTotal number of variables............................:       10\n                     variables with only lower bounds:        0\n                variables with lower and upper bounds:        0\n                     variables with only upper bounds:        0\nTotal number of equality constraints.................:        8\nTotal number of inequality constraints...............:        0\n        inequality constraints with only lower bounds:        0\n   inequality constraints with lower and upper bounds:        0\n        inequality constraints with only upper bounds:        0\n\niter    objective    inf_pr   inf_du inf_compl lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n   0  2.0570000e+03 2.48e+01 1.00e+02 3.96e-04  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n   1  2.0474072e+03 2.47e+01 2.97e+01 4.03e-02  -1.0 2.27e+00    -  1.00e+00 4.00e-03h  1\n   2  1.1009058e+03 1.49e+01 2.24e+01 2.02e-02  -1.0 2.24e+00    -  1.00e+00 1.00e+00h  1\n   3  1.1598224e+02 2.15e+00 5.34e+01 2.00e-02  -1.0 2.14e+00    -  1.00e+00 1.00e+00h  1\n   4  6.5263509e+00 1.12e-01 4.74e+00 2.00e-02  -1.0 1.72e-01    -  1.00e+00 1.00e+00h  1\n   5  6.2326771e+00 1.64e-03 2.08e-02 2.00e-02  -1.0 5.91e-02    -  1.00e+00 1.00e+00h  1\n   6  6.2324576e+00 1.18e-06 1.22e-05 3.87e-04  -3.8 1.40e-03    -  9.98e-01 1.00e+00h  1\n   7  6.2323021e+00 5.36e-11 1.98e-06 8.81e-05  -5.0 3.12e-05    -  8.90e-01 1.00e+00h  1\n\nNumber of Iterations....: 7\n\n                                   (scaled)                 (unscaled)\nObjective...............:   7.8690682927808819e-01    6.2323020878824593e+00\nDual infeasibility......:   1.9831098306832828e-06    1.5706229859011604e-05\nConstraint violation....:   5.3644534994679027e-11    5.3644534994679027e-11\nComplementarity.........:   1.1122043961248693e-05    8.8086588173089670e-05\nOverall NLP error.......:   8.8086588173089670e-05    8.8086588173089670e-05\n\nNumber of objective function evaluations             = 8\nNumber of objective gradient evaluations             = 8\nNumber of constraint evaluations                     = 8\nNumber of constraint Jacobian evaluations            = 8\nNumber of Lagrangian Hessian evaluations             = 7\nTotal wall-clock secs in solver (w/o fun. eval./lin. alg.)  =  2.472\nTotal wall-clock secs in linear solver                      =  0.018\nTotal wall-clock secs in NLP function evaluations           =  0.006\nTotal wall-clock secs                                       =  2.496\n\nEXIT: Optimal Solution Found (tol = 1.0e-04).\n\n\nAgain, only scalar objective/constraints created via @constraint and @objective API are supported. Older syntax like @NLconstraint and @NLobjective are not supported.\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"gpu/#CPU-and-GPU-acceleration","page":"CPU and GPU acceleration","title":"CPU and GPU acceleration","text":"One of the key features of ExaModels.jl is being able to evaluate derivatives either on multi-threaded CPUs or GPU accelerators. Currently, GPU acceleration is only tested for NVIDIA GPUs. If you'd like to use multi-threaded CPU acceleration, start julia with\n\n$ julia -t 4 # using 4 threads\n\nAlso, if you're using NVIDIA GPUs, make sure to have installed appropriate drivers.\n\nLet's say that our CPU code is as follows.\n\nfunction luksan_vlcek_obj(x, i)\n    return 100 * (x[i-1]^2 - x[i])^2 + (x[i-1] - 1)^2\nend\n\nfunction luksan_vlcek_con(x, i)\n    return 3x[i+1]^3 + 2 * x[i+2] - 5 + sin(x[i+1] - x[i+2])sin(x[i+1] + x[i+2]) + 4x[i+1] -\n           x[i]exp(x[i] - x[i+1]) - 3\nend\n\nfunction luksan_vlcek_x0(i)\n    return mod(i, 2) == 1 ? -1.2 : 1.0\nend\n\nfunction luksan_vlcek_model(N)\n\n    c = ExaCore()\n    x = variable(c, N; start = (luksan_vlcek_x0(i) for i = 1:N))\n    constraint(c, luksan_vlcek_con(x, i) for i = 1:(N-2))\n    objective(c, luksan_vlcek_obj(x, i) for i = 2:N)\n\n    return ExaModel(c)\nend\n\nluksan_vlcek_model (generic function with 1 method)\n\nNow we simply modify this by\n\nfunction luksan_vlcek_model(N, backend = nothing)\n\n    c = ExaCore(; backend = backend) # specify the backend\n    x = variable(c, N; start = (luksan_vlcek_x0(i) for i = 1:N))\n    constraint(c, luksan_vlcek_con(x, i) for i = 1:(N-2))\n    objective(c, luksan_vlcek_obj(x, i) for i = 2:N)\n\n    return ExaModel(c)\nend\n\nluksan_vlcek_model (generic function with 2 methods)\n\nThe acceleration can be done simply by specifying the backend. In particular, for multi-threaded CPUs,\n\nusing ExaModels, NLPModelsIpopt, KernelAbstractions\n\nm = luksan_vlcek_model(10, CPU())\nipopt(m)\n\n\"Execution stats: first-order stationary\"\n\nFor NVIDIA GPUs, we can use CUDABackend. However, currently, there are not many optimization solvers that are capable of solving problems on GPUs. The only option right now is using MadNLP.jl. To use this, first install\n\nimport Pkg; Pkg.add(\"MadNLPGPU\")\n\nThen, we can run:\n\nusing CUDA, MadNLPGPU\n\nm = luksan_vlcek_model(10, CUDABackend())\nmadnlp(m)\n\nIn the case we have arrays for the data, what we need to do is to simply convert the array types to the corresponding device array types. In particular,\n\nfunction cuda_luksan_vlcek_model(N)\n    c = ExaCore(; backend = CUDABackend())\n    d1 = CuArray(1:(N-2))\n    d2 = CuArray(2:N)\n    d3 = CuArray([luksan_vlcek_x0(i) for i = 1:N])\n\n    x = variable(c, N; start = d3)\n    constraint(c, luksan_vlcek_con(x, i) for i in d1)\n    objective(c, luksan_vlcek_obj(x, i) for i in d2)\n\n    return ExaModel(c)\nend\n\ncuda_luksan_vlcek_model (generic function with 1 method)\n\nm = cuda_luksan_vlcek_model(10)\nmadnlp(m)\n\nSince ExaModels builds on KernelAbstractions.jl, it can in principle target multiple hardware backends. The following backends are provided by the JuliaGPU ecosystem:\n\nCPU() for multi-threaded CPU execution\nCUDABackend() for NVIDIA GPUs (CUDA.jl)\nROCBackend() for AMD GPUs (AMDGPU.jl)\nOneAPIBackend() for Intel GPUs (oneAPI.jl)\nMetalBackend() for Apple GPUs (Metal.jl)\nOpenCLBackend() for generic OpenCL devices (OpenCL.jl)\n\n\n\nThis page was generated using Literate.jl.","category":"section"},{"location":"ref/#References","page":"References","title":"References","text":"L. T. Biegler. Nonlinear programming: concepts, algorithms, and applications to chemical processes (SIAM, 2010).\n\n\n\nC. Coffrin, R. Bent, K. Sundar, Y. Ng and M. Lubin. PowerModels.jl: An open-source framework for exploring power flow formulations. In: 2018 Power Systems Computation Conference (PSCC) (IEEE, 2018); pp. 1–8.\n\n\n\nL. Lukšan and J. Vlček. Indefinitely preconditioned inexact Newton method for large sparse equality constrained non-linear programming problems. Numerical linear algebra with applications 5, 219–247 (1998).\n\n\n\n","category":"section"},{"location":"#Introduction","page":"Introduction","title":"Introduction","text":"Welcome to the documentation of ExaModels.jl\t\n\nnote: Note\nExaModels runs on julia VERSION ≥ v\"1.9\"\n\nwarning: Warning\nPlease help us improve ExaModels and this documentation! ExaModels is in the early stage of development, and you may encounter unintended behaviors or missing documentations. If you find anything is not working as intended or documentation is missing, please open issues or pull requests or start discussions. ","category":"section"},{"location":"#What-is-ExaModels.jl?","page":"Introduction","title":"What is ExaModels.jl?","text":"ExaModels.jl is an algebraic modeling and automatic differentiation tool in Julia Language, specialized for SIMD abstraction of nonlinear programs. ExaModels.jl employs what we call SIMD abstraction for nonlinear programs (NLPs), which allows for the preservation of the parallelizable structure within the model equations, facilitating efficient automatic differentiation either on the single-thread CPUs, multi-threaded CPUs, as well as GPU accelerators. More details about SIMD abstraction can be found here.","category":"section"},{"location":"#Key-differences-from-other-algebraic-modeling-tools","page":"Introduction","title":"Key differences from other algebraic modeling tools","text":"ExaModels.jl is different from other algebraic modeling tools, such as JuMP or AMPL, in the following ways:\n\nModeling Interface: ExaModels.jl requires users to specify the model equations always in the form of Generators. This restrictive structure allows ExaModels.jl to preserve the SIMD-compatible structure in the model equations. This unique feature distinguishes ExaModels.jl from other algebraic modeling tools.\nPerformance: ExaModels.jl compiles (via Julia's compiler) derivative evaluation codes tailored to each computation pattern. Through reverse-mode automatic differentiation using these tailored codes, ExaModels.jl achieves significantly faster derivative evaluation speeds, even when using CPU.\nPortability: ExaModels.jl goes beyond traditional boundaries of\n\nalgebraic modeling systems by enabling derivative evaluation on GPU accelerators. Implementation of GPU kernels is accomplished using the portable programming paradigm offered by KernelAbstractions.jl. With ExaModels.jl, you can run your code on various devices, including multi-threaded CPUs, NVIDIA GPUs, AMD GPUs, and Intel GPUs. Note that Apple's Metal is currently not supported due to its lack of support for double-precision arithmetic.\n\nThus, ExaModels.jl shines when your model has\n\nnonlinear objective and constraints;\na large number of variables and constraints;\nhighly repetitive structure;\nsparse Hessian and Jacobian.\n\nThese features are often exhibited in optimization problems associated with first-principle physics-based models. Primary examples include optimal control problems formulated with direct subscription method [1] and network system optimization problems, such as optimal power flow [2] and gas network control/estimation problems.","category":"section"},{"location":"#Performance-Highlights","page":"Introduction","title":"Performance Highlights","text":"ExaModels.jl significantly enhances the performance of derivative evaluations for nonlinear optimization problems that can benefit from SIMD abstraction. Recent benchmark results demonstrate this notable improvement. Notably, when solving the AC OPF problem for a 9241 bus system, derivative evaluation using ExaModels.jl on GPUs can be up to two orders of magnitude faster compared to JuMP or AMPL. Some benchmark results are available below. The following  problems are used for benchmarking:\n\nLuksanVlcek problem\nQuadrotor control problem\nDistillation column control problem\nAC optimal power flow problem\n\n(Image: benchmark)","category":"section"},{"location":"#Supported-Solvers","page":"Introduction","title":"Supported Solvers","text":"ExaModels can be used with any solver that can handle NLPModel data type, but several callbacks are not currently implemented, and cause some errors. Currently, it is tested with the following solvers:\n\nIpopt (via NLPModelsIpopt.jl)\nMadNLP.jl","category":"section"},{"location":"#Documentation-Structure","page":"Introduction","title":"Documentation Structure","text":"This documentation is structured in the following way.\n\nThe remainder of this page highlights several key aspects of ExaModels.jl.\nThe mathematical abstraction–-SIMD abstraction of nonlinear programming–-of ExaModels.jl is discussed in Mathematical Abstraction page.\nThe step-by-step tutorial of using ExaModels.jl can be found in Tutorial page.\nThis documentation does not intend to discuss the engineering behind the implementation of ExaModels.jl. Some high-level idea is discussed in a recent publication, but the full details of the engineering behind it will be discussed in the future publications.","category":"section"},{"location":"#Citing-ExaModels.jl","page":"Introduction","title":"Citing ExaModels.jl","text":"If you use ExaModels.jl in your research, we would greatly appreciate your citing this preprint.\n\n@misc{shin2023accelerating,\n      title={Accelerating Optimal Power Flow with {GPU}s: {SIMD} Abstraction of Nonlinear Programs and Condensed-Space Interior-Point Methods}, \n      author={Sungho Shin and Fran{\\c{c}}ois Pacaud and Mihai Anitescu},\n      year={2023},\n      eprint={2307.16830},\n      archivePrefix={arXiv},\n      primaryClass={math.OC}\n}","category":"section"},{"location":"#Supporting-ExaModels.jl","page":"Introduction","title":"Supporting ExaModels.jl","text":"Please report issues and feature requests via the GitHub issue tracker.\nQuestions are welcome at GitHub discussion forum.","category":"section"}]
}
